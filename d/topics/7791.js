var data={"title":"Glossary of common biostatistical and epidemiological terms","body":"<div id=\"topicContent\" class=\"utdArticleSection utdStyle\"><div id=\"topicTitle\">Glossary of common biostatistical and epidemiological terms</div><dl id=\"topicContributors\"><dt><span> </span>Author:</dt><dd><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/contributors\" class=\"contributor contributor_credentials\">Peter A L Bonis, MD</a></dd><dt><span> </span>Section Editors:</dt><dd><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/contributors\" class=\"contributor contributor_credentials\">Joann G Elmore, MD, MPH</a></dd><dd><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/contributors\" class=\"contributor contributor_credentials\">David M Rind, MD</a></dd><dt><span> </span>Deputy Editor:</dt><dd><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/contributors\" class=\"contributor contributor_credentials\">Carrie Armsby, MD, MPH</a></dd></dl><p class=\"disclosureLink\"><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/contributor-disclosure\" class=\"contributor contributor_credentials\">Contributor Disclosures</a></p><div id=\"reviewProcess\"><span>All topics are updated as new evidence becomes available and our <a href=\"https://www.uptodate.com/home/editorial-policy\" target=\"_blank\" class=\"policy policy_editorialpolicy\">peer review process</a> is complete.</span></div><div id=\"literatureReviewDate\"><span class=\"emphasis\">Literature review current through:</span>&#160;Mar 2018.&#160;&#124;&#160;<span class=\"emphasis\">This topic last updated:</span>&#160;Feb 22, 2016.</div><div id=\"topicWhatsNewContainer\"></div><div id=\"topicText\"><p class=\"headingAnchor\" id=\"H1\"><span class=\"h1\">INTRODUCTION</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>This topic review will provide a catalog of common biostatistical and epidemiological terms encountered in the medical literature.</p><p class=\"headingAnchor\" id=\"H2\"><span class=\"h1\">STATISTICS THAT DESCRIBE HOW DATA ARE DISTRIBUTED</span></p><p class=\"headingAnchor\" id=\"H3\"><span class=\"h2\">Measures of central tendency</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Three measures of central tendency are most frequently used to describe data:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Mean equals the sum of observations divided by the number of observations.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Median equals the observation in the middle when all observations are ordered from smallest to largest; when there are an even number of observations the median is defined as the mean of the middle two data points.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Mode equals the observation that occurs most frequently.</p><p/><p class=\"headingAnchor\" id=\"H4\"><span class=\"h2\">Measures of dispersion</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Dispersion refers to the degree to which data are scattered around a specific value (such as the mean). The most commonly used measures of dispersion are:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Range &ndash; The range equals the difference between the largest and smallest observation.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Standard deviation &ndash; The standard deviation measures the variability of data around the mean. It provides information on how much variability can be expected among individuals within a population. In samples that follow a &quot;normal&quot; distribution (ie, Gaussian), 68 and 95 percent of values fall within one and two standard deviations of the mean, respectively.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Standard error of the mean &ndash; Standard deviation of the mean (for a sample population) should be distinguished from the standard error of the mean, which describes how much variability can be expected when measuring the mean from several different samples.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Percentile &ndash; The percentile equals the percentage of a distribution that is below a specific value. As an example, a child is in 90th percentile for weight if only 10 percent of children the same age weigh more than she does.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Interquartile range &ndash; The interquartile range refers to the upper and lower values defining the central 50 percent of observations. The boundaries are equal to the observations representing the 25th and 75th percentiles. The interquartile range is depicted in a box and whiskers plot (<a href=\"image.htm?imageKey=PC%2F57798\" class=\"graphic graphic_figure graphicRef57798 \">figure 1</a>).</p><p/><p class=\"headingAnchor\" id=\"H5\"><span class=\"h1\">TERMS USED TO DESCRIBE THE FREQUENCY OF AN EVENT</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Incidence and prevalence are the two main terms used to describe the frequency of an event.</p><p class=\"headingAnchor\" id=\"H6\"><span class=\"h2\">Incidence</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Incidence represents the number of new events that have occurred in a specific time interval divided by the population at risk at the beginning of the time interval. The result gives the likelihood of developing an event in that time interval.</p><p class=\"headingAnchor\" id=\"H7\"><span class=\"h2\">Prevalence</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Prevalence refers to the number of individuals with a given disease at a given point in time divided by the population at risk at that point in time. Prevalence has been further defined as being &quot;point&quot; or &quot;period.&quot; Point prevalence refers to the proportion of individuals with a condition at a specified point in time, while period prevalence refers to the proportion of individuals with a condition during a specified interval (eg, a year).</p><p class=\"headingAnchor\" id=\"H8\"><span class=\"h2\">Person-years</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Person years refers to the total number of years that each member of a study population has been under observation or treatment. It is obtained by multiplying the number of years by the number of members of a sample population who have had a certain condition or undergone a particular treatment.</p><p class=\"headingAnchor\" id=\"H9\"><span class=\"h1\">TERMS USED TO DESCRIBE THE MAGNITUDE OF AN EFFECT</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The types of descriptors used to define the relationship among variables of interest in a data set and the effect of one variable on another depend upon the type of data. Important examples are the relative risk and odds ratio, which are commonly encountered expressions describing the relationship between nominal characteristics (ie, variables that are grouped as unique categories) (<a href=\"image.htm?imageKey=PC%2F71957\" class=\"graphic graphic_figure graphicRef71957 \">figure 2</a>).</p><p class=\"headingAnchor\" id=\"H10\"><span class=\"h2\">Relative risk</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The relative risk (or risk ratio) equals the incidence in exposed individuals divided by the incidence in unexposed individuals. The relative risk can be calculated from studies in which the proportion of patients exposed and unexposed to a risk is known, such as a cohort study. (See <a href=\"#H33\" class=\"local\">'Cohort study'</a> below.)</p><p class=\"headingAnchor\" id=\"H11\"><span class=\"h2\">Odds ratio</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The odds ratio equals the odds that an individual with a specific condition has been exposed to a risk factor divided by the odds that a control has been exposed. The odds ratio is used in case-control studies and is often generated in multivariate analyses as well (see <a href=\"#H34\" class=\"local\">'Case-control study'</a> below). The odds ratio provides a reasonable estimate of the relative risk for uncommon conditions (<a href=\"image.htm?imageKey=PC%2F71957\" class=\"graphic graphic_figure graphicRef71957 \">figure 2</a>).</p><p>The relative risk and odds ratio are interpreted relative to the number one. An odds ratio of 0.6, for example, suggests that patients exposed to a variable of interest were 40 percent less likely to develop a specific outcome compared to the control group. Similarly, an odds ratio of 1.5 suggests that the risk was increased by 50 percent.</p><p class=\"headingAnchor\" id=\"H12\"><span class=\"h2\">Absolute risk</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The relative risk and odds ratio provide an understanding of the magnitude of risk compared with a standard. However, it is more often desirable to know information about the absolute risk (also known as risk difference). As an example, a 40 percent increase in mortality due to a particular exposure does not provide direct insight into the likelihood that exposure in an individual patient will lead to mortality. In some cases, a large relative risk reduction may not be clinically important. A 50 percent reduction in an outcome, for example, is much more impressive if the baseline rate of the outcome is 25 percent compared with 1 percent.</p><p>The &quot;attributable risk&quot; represents the difference in the rate of a disease in an exposed, compared with a non-exposed, population. It reflects the additional incidence of disease related to an exposure taking into account the background rate of the disease. The attributable risk is calculated by subtracting the incidence of a disease in non-exposed persons from the incidence of disease in exposed persons.</p><p>A related term, the &quot;population attributable risk&quot; is used to describe the contribution that an exposure has on the incidence of a specific disease in a population. It is calculated by multiplying the attributable risk by the prevalence of exposure to a risk factor in a population. The population attributable risk is particularly important when considering public health measures and the allocation of resources intended to reduce the incidence of a disease.</p><p class=\"headingAnchor\" id=\"H13\"><span class=\"h2\">Number needed to treat</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The benefit of an intervention can be expressed by the &quot;number needed to treat&quot; (NNT). NNT is the reciprocal of the absolute risk reduction (the absolute adverse event rate for placebo minus the absolute adverse event rate for treated patients). Its interpretation can be illustrated by the following sentence: &quot;This study suggests that I would have to treat five patients with a drug to prevent one death.&quot;</p><p>As an example, consider a placebo-controlled trial involving 100 patients. Thirty patients died during the study period (10 receiving active drug and 20 receiving placebo) giving a mortality rate of 20 percent with active drug (10 divided by (10 +40)) versus 40 percent (20 divided by (20 + 30)) with placebo as shown in the left panel of the figure (<a href=\"image.htm?imageKey=PEDS%2F53580\" class=\"graphic graphic_figure graphicRef53580 \">figure 3</a>). The difference between these two rates, the &quot;risk difference&quot;, is used to calculate NNT.</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>40 percent minus 20 percent = 20 percent = 0.2</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>1 divided by 0.2 = 5</p><p/><p>Thus, this study suggests that only five patients need to be treated with the drug (compared with placebo) to prevent one death.</p><p>Because it is intuitive, the NNT has become an increasingly popular expression of absolute benefit or risk, potentially allowing for comparison of the relative benefit (or harm) of different interventions. However, the NNT can be misleading:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>It implies that the option is to treat or not to treat rather than to treat or switch to another more effective treatment [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/1\" class=\"abstract_t\">1</a>].</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>There are variations on how NNT is determined; NNTs from different studies cannot be compared unless the methods used to determine them are identical [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/2\" class=\"abstract_t\">2</a>]. This may be a particular consideration when NNTs are calculated for treatment of chronic diseases in which outcomes (such as mortality) do not cluster in time.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Calculation of the NNT depends upon the control rate (ie, the rate of events in the control arm). The control rate can be variable (particularly in small controlled trials, which are more vulnerable to random effects). As a result, the NNT may not accurately reflect the benefit of an intervention if events occurred in the control arm more or less than would be expected based upon the biology of the disease. This effect can be particularly problematic when comparing the NNTs among placebo controlled trials (<a href=\"image.htm?imageKey=PEDS%2F53580\" class=\"graphic graphic_figure graphicRef53580 \">figure 3</a>) [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/3\" class=\"abstract_t\">3</a>].</p><p/><p>When the outcome is a harm rather than a benefit, a number needed to harm (NNH) can be calculated similarly. As an example, if statin therapy causes myalgias in 5 percent of patients, then treating 20 patients with statins would be expected to cause one case of myalgia, for an NNH of 20. Other variations that sometimes appear in the medical literature include number needed to prevent and number needed to diagnose.</p><p class=\"headingAnchor\" id=\"H14\"><span class=\"h1\">TERMS USED TO DESCRIBE THE QUALITY OF MEASUREMENTS</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The most commonly used measures to describe the quality of an observation are reliability and validity.</p><p class=\"headingAnchor\" id=\"H15\"><span class=\"h2\">Reliability</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Reliability refers to the extent to which repeated measurements of a relatively stable phenomenon fall closely to each other. Several different types of reliability can be measured, such as inter- and intraobserver reliability and test-retest reliability. </p><p>As an example, the kappa statistic is a measure of the agreement between two observers who independently measure the same data. It can range from -1.0 to +1.0. If there is perfect agreement, the value is 1.0, whereas if the observed agreement is what would be expected by chance alone, the value is zero. If the degree of agreement is worse than what would be expected by chance, the kappa value will be negative, with complete disagreement resulting in a value of -1.0. Kappa statistics are often interpreted as:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Excellent agreement &ndash; 0.8 to 1.0</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Good agreement &ndash; 0.6 to 0.8</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Moderate agreement &ndash; 0.4 to 0.6</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Fair agreement &ndash; 0.2 to 0.4</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Poor agreement &ndash; Less than 0.2</p><p/><p class=\"headingAnchor\" id=\"H16\"><span class=\"h2\">Validity</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Validity refers to the extent to which an observation reflects the &quot;truth&quot; of the phenomenon being measured. Several types can be measured. In studies involving questionnaire development, for example, types of validity include content (the extent to which the measure reflects the dimensions of a particular problem), construct (the extent to which a measure is affirmed by an external established indicator), and criterion validity (the extent to which a measure can predict an observable phenomenon). These types of validity are useful since the &quot;truth&quot; may not be physically verifiable.</p><p class=\"headingAnchor\" id=\"H17\"><span class=\"h1\">MEASURES OF DIAGNOSTIC TEST PERFORMANCE</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The most common terms used to describe the performance of a diagnostic test are sensitivity and specificity (<a href=\"image.htm?imageKey=GAST%2F77832\" class=\"graphic graphic_table graphicRef77832 \">table 1</a>). (See <a href=\"topic.htm?path=evaluating-diagnostic-tests\" class=\"medical medical_review\">&quot;Evaluating diagnostic tests&quot;</a>.)</p><p class=\"headingAnchor\" id=\"H18\"><span class=\"h2\">Sensitivity</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The number of patients with a positive test who have a disease divided by all patients who have the disease. A test with high sensitivity will not miss many patients who have the disease (ie, few false negative results).</p><p class=\"headingAnchor\" id=\"H19\"><span class=\"h2\">Specificity</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The number of patients who have a negative test and do not have the disease divided by the number of patients who do not have the disease. A test with high specificity will infrequently identify patients as having a disease when they do not (ie, few false positive results).</p><p>Sensitivity and specificity are properties of tests that should be considered when tests are obtained. In addition, sensitivity and specificity are interdependent. Thus, for a given test, an increase in sensitivity is accompanied by a decrease in specificity and vice versa. This can be illustrated by the following example. Consider two populations of patients: one has chronic hepatitis as defined by a reference standard such as a liver biopsy, and the other does not. The diagnostic test being used to evaluate for chronic hepatitis is the serum alanine aminotransferase (ALT) concentration. The sensitivity and specificity of the ALT depend upon the value chosen as a cutoff (<a href=\"image.htm?imageKey=PC%2F72237\" class=\"graphic graphic_figure graphicRef72237 \">figure 4</a>).</p><p>The interdependence of sensitivity and specificity can be depicted graphically using a receiver operating characteristic (ROC) curve. The ROC curve plots sensitivity on the Y axis, and 1-specificity (which is the false positive rate) on the X axis. The area under the ROC curves (the area to the right and below the curve) gives an estimate of the accuracy of a test. An ideal test would have a cutoff value that perfectly discriminated those with disease, and would have an area under the ROC curve of 1.00 (<a href=\"image.htm?imageKey=PC%2F54239\" class=\"graphic graphic_figure graphicRef54239 \">figure 5</a>). The ROC curve can be adapted to multivariate analysis (such as logistic regression) in which it provides an estimate of the accuracy of the statistical model (ie, how well it predicts an outcome).</p><p class=\"headingAnchor\" id=\"H20\"><span class=\"h2\">Predictive values</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>In addition to sensitivity and specificity, the predictive values of a diagnostic test must be considered when interpreting the results of a test (<a href=\"topic.htm?path=calculator-post-test-probability-from-pre-test-probability-sensitivity-and-specificity\" class=\"calc calc_professional\">calculator 1</a>). The positive predictive value of a test represents the likelihood that a patient with a positive test has the disease. Conversely, the negative predictive value represents the likelihood that a patient who has a negative test is free of the disease (<a href=\"image.htm?imageKey=GAST%2F77832\" class=\"graphic graphic_table graphicRef77832 \">table 1</a>).</p><p>The predictive values (and the proportion of positive and negative evaluations that can be expected) depend upon the prevalence of a disease within a population. Thus, for given values of sensitivity and specificity, a patient with a positive test is more likely to truly have the disease if the patient belongs to a population with a high prevalence of the disease (<a href=\"image.htm?imageKey=PC%2F68072\" class=\"graphic graphic_figure graphicRef68072 \">figure 6</a>).</p><p>This observation has significant implications for screening tests, in which false positive results may lead to expensive and sometimes dangerous testing, and false negative tests may be associated with morbidity or mortality. As an example, a positive stool test for occult blood is much more likely to predict colon cancer in a seventy year-old compared with a twenty year-old. Thus, routine screening of stools in young patients would lead to a high rate of subsequent false positive examinations and is not recommended. The predictive values of a test should be considered when selecting among diagnostic tests for an individual patient in whom demographic or other clinical risk factors influence the likelihood that the disease is present (ie, the &quot;prior probability&quot; of the disease).</p><p class=\"headingAnchor\" id=\"H21\"><span class=\"h2\">Likelihood ratio</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>As discussed above, a limitation to predictive values as expressions of test characteristics is their dependence upon disease prevalence. To overcome this limitation, the likelihood ratio has been used as an expression of the performance of diagnostic tests [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/4\" class=\"abstract_t\">4</a>]. Likelihood ratios are an expression of sensitivity and specificity that can be used to estimate the odds that a condition is present or absent (<a href=\"topic.htm?path=calculator-post-test-probability-from-likelihood-ratios-and-multiple-test-results\" class=\"calc calc_professional\">calculator 2</a>). (See <a href=\"topic.htm?path=evaluating-diagnostic-tests\" class=\"medical medical_review\">&quot;Evaluating diagnostic tests&quot;</a>.)</p><p>The likelihood ratio represents a measure of the odds of having a disease relative to the prior probability of the disease. The estimate is independent of the disease prevalence. A positive likelihood ratio is calculated by dividing sensitivity by 1 minus specificity <span class=\"nowrap\">(sensitivity/(1-specificity))</span>. Similarly, a negative likelihood ratio is calculated by dividing 1 minus sensitivity by specificity <span class=\"nowrap\">((1-sensitivity)/specificity)</span>. Positive and negative likelihood ratios of 9 and 0.25, for example, can be interpreted as meaning that a positive result is seen 9 times as frequently while a negative test is seen 0.25 times as frequently in those with a specific condition than those without it. Likelihood ratios can be established for many cutoff points for a diagnostic test permitting an appreciation for the relative importance of a large versus small increase in a test result.</p><p class=\"headingAnchor\" id=\"H22\"><span class=\"h2\">Accuracy</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The performance of a diagnostic test is sometimes expressed as accuracy, which refers to the number of true positives and true negatives divided by the total number of observations (<a href=\"image.htm?imageKey=PC%2F55297\" class=\"graphic graphic_table graphicRef55297 \">table 2</a>). However, accuracy by itself is not a good indicator of test performance since it obscures important information related to its component parts.</p><p class=\"headingAnchor\" id=\"H1432545586\"><span class=\"h2\">Net reclassification improvement and integrated discrimination improvement</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Net reclassification improvement (NRI) is a method used for evaluating improvements in risk predictions from diagnostic tests and prediction models. It attempts to quantify the extent to which the addition of a diagnostic test or prediction model will influence clinical practice. Typical use of the NRI would be to depict the extent to which a new model correctly classifies patients (eg, for risk of dying) compared with the old model. While appealing, there is variability in how the NRI is calculated and reported [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/5\" class=\"abstract_t\">5</a>].</p><p>Another measure, integrated discrimination improvement (IDI), also attempts to provide a quantitative view on how much value a new diagnostic test or prediction rule provides [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/6\" class=\"abstract_t\">6</a>].</p><p class=\"headingAnchor\" id=\"H23\"><span class=\"h1\">EXPRESSIONS USED WHEN MAKING INFERENCES ABOUT DATA</span></p><p class=\"headingAnchor\" id=\"H24\"><span class=\"h2\">Confidence interval</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A point estimate (ie, a single value) from a sample population may not reflect the &quot;true&quot; value from the entire population. As a result, it is often helpful to provide a range that is likely to include the true value. A confidence interval is a commonly used estimate. The boundaries of a confidence interval give values within which there is a high probability (95 percent by convention) that the true population value can be found. The calculation of a confidence interval considers the standard deviation of the data and the number of observations. Thus, a confidence interval narrows as the number of observations increases, or its variance (dispersion) decreases. The interpretation of confidence intervals is discussed in more detail separately. (See <a href=\"topic.htm?path=proof-p-values-and-hypothesis-testing#H8\" class=\"medical medical_review\">&quot;Proof, p-values, and hypothesis testing&quot;, section on 'Confidence intervals'</a>.)</p><p class=\"headingAnchor\" id=\"H25\"><span class=\"h2\">Credible interval</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A credible interval is used in Bayesian analysis to describe the range in which a posterior probability estimate is likely to reside. As an example, a 95 percent credible interval for a posterior probability estimate of 40 percent could range from 30 to 50 percent, indicating that there is a 95 percent chance that the true posterior probability estimate lies within the 30 to 50 percent range. There are fundamental differences in how credible intervals are derived compared with the more commonly-used confidence intervals. Nevertheless, their intuitive interpretation is similar.</p><p class=\"headingAnchor\" id=\"H26\"><span class=\"h2\">Errors</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Two potential errors are commonly recognized when testing a hypothesis:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>A type I error (also referred to as an &quot;alpha error&quot;) is incorrectly concluding that there is a statistically significant difference in a dataset; the probability of making a type I error is called &quot;alpha&quot;. A typical value for alpha is 0.05. Thus, a p&lt;0.05 leads to a decision to reject the null hypothesis. (See <a href=\"topic.htm?path=proof-p-values-and-hypothesis-testing#H7\" class=\"medical medical_review\">&quot;Proof, p-values, and hypothesis testing&quot;, section on 'P-values'</a>.)</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>A type II error (also referred to as a &quot;beta error&quot;) is incorrectly concluding that there was no statistically significant difference in a dataset; the probability of making a type II error is called &quot;beta.&quot; This error often reflects insufficient power of the study.</p><p/><p class=\"headingAnchor\" id=\"H27\"><span class=\"h2\">Power</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The term &quot;power&quot; (calculated as 1 - beta) refers to the ability of a study to detect a true difference. Negative findings in a study may reflect that the study was underpowered to detect a difference. A &quot;power calculation&quot; should be performed prior to conducting a study to be sure that there are a sufficient number of observations to detect a desired degree of difference. The larger the difference, the fewer the number of observations that will be required. As an example, it takes fewer patients to detect a 50 percent difference in blood pressure from a new antihypertensive medication compared with placebo than a 5 percent difference. The interpretation of power calculations is discussed in more detail separately. (See <a href=\"topic.htm?path=proof-p-values-and-hypothesis-testing#H10\" class=\"medical medical_review\">&quot;Proof, p-values, and hypothesis testing&quot;, section on 'Power in a negative study'</a>.)</p><p class=\"headingAnchor\" id=\"H28\"><span class=\"h1\">TERMS USED IN MULTIVARIATE ANALYSIS</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The effect of more than one variable often needs to be considered when predicting an outcome. As an example, the effect of smoking status and age needs to be simultaneously considered when assessing the risk of lung cancer.</p><p>Statistical methods that can simultaneously account for multiple variables are known as &quot;multivariate&quot; (or multivariable) analysis. These methods help to &quot;control&quot; (or &quot;adjust&quot;) for variables that are extraneous to the main causal question and might confound it. A commonly encountered form of multivariable analysis, logistic regression, is applied to models in which the outcome is dichotomous (eg, alive or dead, or a complication occurs or does not occur).</p><p class=\"headingAnchor\" id=\"H29\"><span class=\"h1\">TIME-TO-EVENT ANALYSIS (SURVIVAL ANALYSIS)</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Many examples of medical research deal with an event that may or may not occur in a given period of time (such as death, stroke, myocardial infarction). During the study, several outcomes are possible in addition to the outcome of interest (eg, patients might die of other causes or drop out from the analysis). Furthermore, the duration of follow-up can vary among individuals in the study. A patient who is observed for five years should count more in the statistical analysis than one observed for five months.</p><p>Several methods are available to account for these considerations. The most commonly used in medical research are Kaplan-Meier and Cox proportional hazards analyses.</p><p class=\"headingAnchor\" id=\"H30\"><span class=\"h2\">Kaplan-Meier analysis</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Kaplan-Meier analysis measures the ratio of surviving patients (or those free from an outcome) divided by the total number of patients at risk for the outcome. Every time a patient has an outcome, the ratio is recalculated. Using these calculations, a curve can be generated that graphically depicts the probability of survival as time passes (<a href=\"image.htm?imageKey=PC%2F67914\" class=\"graphic graphic_figure graphicRef67914 \">figure 7</a>).</p><p>In many studies, the benefit of a drug or intervention on an outcome is compared with a control population, permitting the construction of two or more Kaplan-Meier curves. Curves that are close together or cross are unlikely to reflect a statistically significant difference. Several formal statistical tests can be used to assess a significant difference. Examples include the log-rank test and the Breslow test.</p><p class=\"headingAnchor\" id=\"H31\"><span class=\"h2\">Cox proportional hazards analysis</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Cox proportional hazards analysis is similar to logistic regression because it can account for many variables that are relevant for predicting a dichotomous outcome. However, unlike logistic regression, Cox proportional hazards analysis permits time to be included as a variable, and for patients to be counted only for the period of time in which they were observed.</p><p>The term &quot;hazard ratio&quot; is sometimes used when referring to variables included in the analysis. A hazard ratio is analogous to an odds ratio. Thus, a hazard ratio of 10 means that group of patients exposed to a specific risk factor has 10 times the chance of developing the outcome compared with unexposed controls.</p><p class=\"headingAnchor\" id=\"H32\"><span class=\"h1\">STUDY DESIGNS</span></p><p class=\"headingAnchor\" id=\"H33\"><span class=\"h2\">Cohort study</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A cohort is a clearly identified group of people to be studied. A cohort study might identify persons specifically because they were or were not exposed to a risk factor, or by taking a random sample of a given population. A cohort study can then move forward to observing the outcome of interest, even if the data are collected retrospectively. As an example, a group of patients who have variable exposure to a risk factor of interest can be followed over time for an outcome.</p><p>The Nurses' Health Study is an example of a cohort study. A large number of nurses are followed over time for an outcome such as colon cancer, providing an estimate of the risk of colon cancer in this population. In addition, dietary intake of various components can be assessed, and the risk of colon cancer in those with high and low intake of fiber can be evaluated to determine if fiber is a risk factor (or a protective factor) for colon cancer. The relative risk of colon cancer in those with high or low fiber intakes can be calculated from such a cohort study. (See <a href=\"#H10\" class=\"local\">'Relative risk'</a> above.)</p><p class=\"headingAnchor\" id=\"H34\"><span class=\"h2\">Case-control study</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A case-control study starts with the outcome of interest and works backward to the exposure. For instance, patients with a disease are identified and compared with controls for exposure to a risk factor. This design does not permit measurement of the proportion of the population who were exposed to the risk factor and then developed or did not develop the disease; thus, the relative risk or the incidence of disease cannot be calculated. However, in case-control studies, the odds ratio provides a reasonable estimate of the relative risk (<a href=\"image.htm?imageKey=PC%2F71957\" class=\"graphic graphic_figure graphicRef71957 \">figure 2</a>). (See <a href=\"#H11\" class=\"local\">'Odds ratio'</a> above.)</p><p>If one were to perform a case-control study to assess the role of dietary fiber in colon cancer as noted above for the cohort study, a group of patients with colon cancer could be compared with matched controls without colon cancer; the fiber intake in the two groups would then be compared. The case-control study is most useful for uncommon diseases in which a very large cohort would be required to accumulate enough cases for analysis.</p><p class=\"headingAnchor\" id=\"H35\"><span class=\"h2\">Randomized controlled trial</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A randomized controlled trial (RCT) is an experimental design in which patients are assigned to two or more interventions. One group of patients is often assigned to a placebo (placebo control) but a randomized trial can involve two active therapies (active control).</p><p>As an example, patients with a prior colonic polyp could be randomly assigned to take a fiber supplement or a placebo supplement to determine with fiber supplementation decreases the risk of developing colon cancer.</p><p>RCTs are generally the only type of study that can adequately control for unmeasured confounders, and are generally the best evidence for proving causality. (See <a href=\"topic.htm?path=proof-p-values-and-hypothesis-testing#H6\" class=\"medical medical_review\">&quot;Proof, p-values, and hypothesis testing&quot;, section on 'Explanation for the results of a study'</a>.)</p><p class=\"headingAnchor\" id=\"H153401402\"><span class=\"h3\">Intention to treat</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The central principle underlying intention-to-treat analysis is that study participants should be analyzed according to the groups in which they were randomized, even if they did not receive or comply with treatment. Such analysis is contrasted to &quot;as treated&quot; (or &quot;per protocol&quot;) analysis in which subjects are analyzed according to the actual treatment they received.</p><p>The theoretical advantage of intention-to-treat analysis is that it preserves the benefits of randomization (ie, assuring that all of the unmeasured factors that could differ in the treatment and control groups remain accounted for in the analysis). For example, it is possible that patients who complied with treatment differed in some important ways than those who did not. Another way to consider the advantage of intention-to-treat analysis is that it better accounts for factors that can influence the outcomes of a prescribed treatment, not just the effects on those who adhered to it. A drug which has serious side-effects but is highly effective, for example, might look favorable in an &quot;as treated&quot; analysis but less favorable in an intention-to-treat analysis if the majority of patients could not tolerate it.</p><p>Although conceptually simple, analyzing according to intention-to-treat principles can be complex. For example, optimal methods to account for subjects who were lost to follow-up remain uncertain [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/7\" class=\"abstract_t\">7</a>]. Such patients could be considered as treatment failures; however, such an approach can be overly punitive for an otherwise promising therapy. Because of these complexities, studies that reported that they performed an intention-to-treat analysis, may not have always done so or have modified the approach in some way [<a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/8\" class=\"abstract_t\">8</a>].</p><p class=\"headingAnchor\" id=\"H36\"><span class=\"h2\">Mendelian randomization</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Mendelian randomization refers to a non-experimental epidemiologic study design that examines the impact of natural genetic variation in the population on the relationship between an environmental exposure and disease. The primary goal is to establish evidence for a causal association between the exposure and disease.</p><p>In this design, subjects comprising the study population are classified according to their genotype at a specific polymorphic locus known to modify the exposure of interest but not directly influence susceptibility to the disease of interest. Because subjects are typically unaware of their genotype status, and because a subject's genotypes are randomly assigned during meiosis (according to Mendel's law of independent assortment), this exposure modification can be considered a form of natural randomization. This is therefore referred to as &quot;Mendelian randomization.&quot; (See <a href=\"topic.htm?path=mendelian-randomization\" class=\"medical medical_review\">&quot;Mendelian randomization&quot;</a>.)</p></div><div id=\"topicAgreement\">Use of UpToDate is subject to the <a href=\"https://www.uptodate.com/legal/license\" class=\"licenseLink\" id=\"sla_in_page\">Subscription and License Agreement</a>.</div><div id=\"references\" class=\"headingAnchor\"><h1>REFERENCES</h1><ol id=\"reference\"><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/1\" class=\"nounderline abstract_t\">Moriarty PM. Relative risk reduction versus number needed to treat as measures of lipid-lowering trial results. Am J Cardiol 1998; 82:505.</a></li><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/2\" class=\"nounderline abstract_t\">Lubsen J, Hoes A, Grobbee D. Implications of trial results: the potentially misleading notions of number needed to treat and average duration of life gained. Lancet 2000; 356:1757.</a></li><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/3\" class=\"nounderline abstract_t\">de Craen AJ, Vickers AJ, Tijssen JG, Kleijnen J. Number-needed-to-treat and placebo-controlled trials. Lancet 1998; 351:310.</a></li><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/4\" class=\"nounderline abstract_t\">Weissler AM. A perspective on standardizing the predictive power of noninvasive cardiovascular tests by likelihood ratio computation: 1. Mathematical principles. Mayo Clin Proc 1999; 74:1061.</a></li><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/5\" class=\"nounderline abstract_t\">Leening MJ, Vedder MM, Witteman JC, et al. Net reclassification improvement: computation, interpretation, and controversies: a literature review and clinician's guide. Ann Intern Med 2014; 160:122.</a></li><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/6\" class=\"nounderline abstract_t\">Pencina MJ, D'Agostino RB Sr, Demler OV. Novel metrics for evaluating improvement in discrimination: net reclassification and integrated discrimination improvement for normal variables and nested models. Stat Med 2012; 31:101.</a></li><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/7\" class=\"nounderline abstract_t\">Porta N, Bonet C, Cobo E. Discordance between reported intention-to-treat and per protocol analyses. J Clin Epidemiol 2007; 60:663.</a></li><li><a href=\"https://www.uptodate.com/contents/glossary-of-common-biostatistical-and-epidemiological-terms/abstract/8\" class=\"nounderline abstract_t\">Gravel J, Opatrny L, Shapiro S. The intention-to-treat approach in randomized controlled trials: are authors saying what they do and doing what they say? Clin Trials 2007; 4:350.</a></li></ol></div><div id=\"topicVersionRevision\">Topic 2759 Version 20.0</div></div>","outline":"<div id=\"outlineSections\"><h2>Topic Outline</h2><ul id=\"innerOutline\"><li><a href=\"#H1\" id=\"outline-link-H1\">INTRODUCTION</a></li><li><a href=\"#H2\" id=\"outline-link-H2\">STATISTICS THAT DESCRIBE HOW DATA ARE DISTRIBUTED</a><ul><li><a href=\"#H3\" id=\"outline-link-H3\">Measures of central tendency</a></li><li><a href=\"#H4\" id=\"outline-link-H4\">Measures of dispersion</a></li></ul></li><li><a href=\"#H5\" id=\"outline-link-H5\">TERMS USED TO DESCRIBE THE FREQUENCY OF AN EVENT</a><ul><li><a href=\"#H6\" id=\"outline-link-H6\">Incidence</a></li><li><a href=\"#H7\" id=\"outline-link-H7\">Prevalence</a></li><li><a href=\"#H8\" id=\"outline-link-H8\">Person-years</a></li></ul></li><li><a href=\"#H9\" id=\"outline-link-H9\">TERMS USED TO DESCRIBE THE MAGNITUDE OF AN EFFECT</a><ul><li><a href=\"#H10\" id=\"outline-link-H10\">Relative risk</a></li><li><a href=\"#H11\" id=\"outline-link-H11\">Odds ratio</a></li><li><a href=\"#H12\" id=\"outline-link-H12\">Absolute risk</a></li><li><a href=\"#H13\" id=\"outline-link-H13\">Number needed to treat</a></li></ul></li><li><a href=\"#H14\" id=\"outline-link-H14\">TERMS USED TO DESCRIBE THE QUALITY OF MEASUREMENTS</a><ul><li><a href=\"#H15\" id=\"outline-link-H15\">Reliability</a></li><li><a href=\"#H16\" id=\"outline-link-H16\">Validity</a></li></ul></li><li><a href=\"#H17\" id=\"outline-link-H17\">MEASURES OF DIAGNOSTIC TEST PERFORMANCE</a><ul><li><a href=\"#H18\" id=\"outline-link-H18\">Sensitivity</a></li><li><a href=\"#H19\" id=\"outline-link-H19\">Specificity</a></li><li><a href=\"#H20\" id=\"outline-link-H20\">Predictive values</a></li><li><a href=\"#H21\" id=\"outline-link-H21\">Likelihood ratio</a></li><li><a href=\"#H22\" id=\"outline-link-H22\">Accuracy</a></li><li><a href=\"#H1432545586\" id=\"outline-link-H1432545586\">Net reclassification improvement and integrated discrimination improvement</a></li></ul></li><li><a href=\"#H23\" id=\"outline-link-H23\">EXPRESSIONS USED WHEN MAKING INFERENCES ABOUT DATA</a><ul><li><a href=\"#H24\" id=\"outline-link-H24\">Confidence interval</a></li><li><a href=\"#H25\" id=\"outline-link-H25\">Credible interval</a></li><li><a href=\"#H26\" id=\"outline-link-H26\">Errors</a></li><li><a href=\"#H27\" id=\"outline-link-H27\">Power</a></li></ul></li><li><a href=\"#H28\" id=\"outline-link-H28\">TERMS USED IN MULTIVARIATE ANALYSIS</a></li><li><a href=\"#H29\" id=\"outline-link-H29\">TIME-TO-EVENT ANALYSIS (SURVIVAL ANALYSIS)</a><ul><li><a href=\"#H30\" id=\"outline-link-H30\">Kaplan-Meier analysis</a></li><li><a href=\"#H31\" id=\"outline-link-H31\">Cox proportional hazards analysis</a></li></ul></li><li><a href=\"#H32\" id=\"outline-link-H32\">STUDY DESIGNS</a><ul><li><a href=\"#H33\" id=\"outline-link-H33\">Cohort study</a></li><li><a href=\"#H34\" id=\"outline-link-H34\">Case-control study</a></li><li><a href=\"#H35\" id=\"outline-link-H35\">Randomized controlled trial</a><ul><li><a href=\"#H153401402\" id=\"outline-link-H153401402\">- Intention to treat</a></li></ul></li><li><a href=\"#H36\" id=\"outline-link-H36\">Mendelian randomization</a></li></ul></li><li><a href=\"#references\">REFERENCES</a></li></ul></div><div><h2>GRAPHICS <a href=\"#\" id=\"viewAllGraphicsLink\">View All</a></h2><div id=\"outlineGraphics\"><ul><li><div id=\"PEDS/2759|FIG\" class=\"openRelatedGraphics\"><a href=\"#\" title=\"FIGURES\">FIGURES</a></div><ul><li><a href=\"image.htm?imageKey=PC/57798\" class=\"graphic graphic_figure\">- Box and whiskers plot</a></li><li><a href=\"image.htm?imageKey=PC/71957\" class=\"graphic graphic_figure\">- Relative risk and odds ratio</a></li><li><a href=\"image.htm?imageKey=PEDS/53580\" class=\"graphic graphic_figure\">- Number needed to treat</a></li><li><a href=\"image.htm?imageKey=PC/72237\" class=\"graphic graphic_figure\">- Interdependence sens spec</a></li><li><a href=\"image.htm?imageKey=PC/54239\" class=\"graphic graphic_figure\">- ROC curve</a></li><li><a href=\"image.htm?imageKey=PC/68072\" class=\"graphic graphic_figure\">- Prevalence on PPV and NPV</a></li><li><a href=\"image.htm?imageKey=PC/67914\" class=\"graphic graphic_figure\">- Kaplan-Meier survival curves</a></li></ul></li><li><div id=\"PEDS/2759|TAB\" class=\"openRelatedGraphics\"><a href=\"#\" title=\"TABLES\">TABLES</a></div><ul><li><a href=\"image.htm?imageKey=GAST/77832\" class=\"graphic graphic_table\">- Definitions of sensitivity, specificity, PPV, and NPV</a></li><li><a href=\"image.htm?imageKey=PC/55297\" class=\"graphic graphic_table\">- Definition of accuracy</a></li></ul></li></ul></div></div><div><h2>CALCULATORS</h2><div id=\"outlineCalculators\"><ul><li class=\"plainItem\"><a href=\"topic.htm?path=calculator-post-test-probability-from-pre-test-probability-sensitivity-and-specificity\" title=\"calculator 1\" class=\"calc calc_professional\">Calculator: Post Test Probability from Pre Test Probability, Sensitivity and Specificity</a></li><li class=\"plainItem\"><a href=\"topic.htm?path=calculator-post-test-probability-from-likelihood-ratios-and-multiple-test-results\" title=\"calculator 2\" class=\"calc calc_professional\">Calculator: Post-test probability from likelihood ratios and multiple test results</a></li></ul></div></div><div><h2>RELATED TOPICS</h2><div id=\"outlineTopics\"><ul><li class=\"plainItem\"><a href=\"topic.htm?path=evaluating-diagnostic-tests\" class=\"medical medical_review\">Evaluating diagnostic tests</a></li><li class=\"plainItem\"><a href=\"topic.htm?path=mendelian-randomization\" class=\"medical medical_review\">Mendelian randomization</a></li><li class=\"plainItem\"><a href=\"topic.htm?path=proof-p-values-and-hypothesis-testing\" class=\"medical medical_review\">Proof, p-values, and hypothesis testing</a></li></ul></div></div>","javascript":null}