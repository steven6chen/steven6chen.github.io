var data={"title":"Systematic review and meta-analysis","body":"<div id=\"topicContent\" class=\"utdArticleSection utdStyle\"><div id=\"topicTitle\">Systematic review and meta-analysis</div><dl id=\"topicContributors\"><dt><span> </span>Authors:</dt><dd><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/contributors\" class=\"contributor contributor_credentials\">Ethan Balk, MD, MPH</a></dd><dd><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/contributors\" class=\"contributor contributor_credentials\">Peter A L Bonis, MD</a></dd><dt><span> </span>Section Editor:</dt><dd><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/contributors\" class=\"contributor contributor_credentials\">Joann G Elmore, MD, MPH</a></dd><dt><span> </span>Deputy Editor:</dt><dd><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/contributors\" class=\"contributor contributor_credentials\">Carrie Armsby, MD, MPH</a></dd></dl><p class=\"disclosureLink\"><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/contributor-disclosure\" class=\"contributor contributor_credentials\">Contributor Disclosures</a></p><div id=\"reviewProcess\"><span>All topics are updated as new evidence becomes available and our <a href=\"https://www.uptodate.com/home/editorial-policy\" target=\"_blank\" class=\"policy policy_editorialpolicy\">peer review process</a> is complete.</span></div><div id=\"literatureReviewDate\"><span class=\"emphasis\">Literature review current through:</span>&#160;Feb 2018.&#160;&#124;&#160;<span class=\"emphasis\">This topic last updated:</span>&#160;Jul 12, 2016.</div><div id=\"topicWhatsNewContainer\"></div><div id=\"topicText\"><p class=\"headingAnchor\" id=\"H2326445\"><span class=\"h1\">INTRODUCTION</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Clinical decisions in medicine ideally should be based upon guidance from a comprehensive assessment of the body of available knowledge. A single clinical trial, even a large one, is seldom sufficient to provide a confident answer to a clinical question. Indeed, one analysis suggested that most research claims are ultimately proven to be incorrect or inaccurate when additional studies have been performed [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/1\" class=\"abstract_t\">1</a>]. At the same time, it is well-established that large randomized controlled trials do not always confirm the results of prior meta-analyses [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/2-4\" class=\"abstract_t\">2-4</a>]. The &quot;truth&quot; needs to be understood by examining all sources of data as critically and objectively as possible.</p><p>There are several potential benefits to performing systematic analysis, which may also include meta-analysis:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Unique aspects to a single randomized trial, involving the participating patient population, the protocol, the setting in which the trial is performed, or the expertise of the involved clinicians, may limit its generalizable to other settings or individual patients. The conclusions of systematic reviews may be more generalizable than single studies.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Combining studies in meta-analyses increases the sample size and generally produces more precise estimates of the effect size (ie, estimates that have smaller confidence intervals) than a single randomized trial.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Clinicians rarely have the time or resources to critically evaluate the body of evidence relevant to a particular clinical question, and a systematic review can facilitate this investigation.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>In contrast to narrative review articles, most systematic reviews focus on a narrow, clearly defined topic and include all eligible studies, not just those chosen by the author.</p><p/><p>Systematic review and meta-analysis are methods to synthesize the available evidence using an explicit, transparent approach that considers the strengths and weaknesses of the individual studies, the populations and interventions, and specific outcomes that were assessed. Individual practitioners, policy-makers, and guideline developers can use well-conducted systematic reviews to determine best patient management decisions. Organizations that develop guidelines can use the results of systematic reviews and meta-analyses to provide evidence-based recommendations for care.</p><p>This topic review will provide an overview of how systematic reviews and meta-analyses are conducted and how they should be interpreted. In addition, it will provide a summary of methodological terms commonly encountered in such studies.</p><p class=\"headingAnchor\" id=\"H2327453\"><span class=\"h1\">TERMINOLOGY</span></p><p class=\"headingAnchor\" id=\"H78553622\"><span class=\"h2\">Systematic review</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A systematic review is a comprehensive summary of all available evidence that meets predefined eligibility criteria to address a specific clinical question or range of questions. It is based upon a rigorous process that incorporates [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/5-7\" class=\"abstract_t\">5-7</a>]:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Systematic identification of studies that have evaluated the same specific research question(s)</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Critical appraisal of the studies, that may involve meta-analysis (see <a href=\"#H78553629\" class=\"local\">'Meta-analysis'</a> below)</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Presentation of key findings</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Explicit discussion of the limitations of the evidence and the review</p><p/><p>Systematic reviews contrast with traditional &quot;narrative&quot; reviews and textbook chapters. Such reviews generally do not exhaustively review the literature, lack transparency in the selection and interpretation of supporting evidence, generally do not provide a quantitative synthesis of the data, and are more likely to be biased [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/8\" class=\"abstract_t\">8</a>].</p><p class=\"headingAnchor\" id=\"H78553629\"><span class=\"h2\">Meta-analysis</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Meta-analysis, which is commonly included in systematic reviews, is a statistical method that quantitatively combines the results from different studies. It can be used to provide an overall estimate of the net benefit or harm of an intervention, even when these effects may not have been apparent in the individual studies [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/9\" class=\"abstract_t\">9</a>]. Meta-analysis can also provide an overall quantitative estimate of other parameters such as diagnostic accuracy, incidence, or prevalence. Meta-regression and network meta-analysis are enhancements to traditional meta-analysis. (See <a href=\"#H2327476\" class=\"local\">'Meta-regression'</a> below and <a href=\"#H2326523\" class=\"local\">'Network meta-analysis'</a> below.)</p><p class=\"headingAnchor\" id=\"H78553636\"><span class=\"h2\">Heterogeneity</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>An important dimension of systematic reviews is that they permit a clear description of differences among studies, typically referred to as &quot;heterogeneity,&quot; and explain the reasons why studies may have reached varied conclusions. The degree of heterogeneity across studies can be expressed graphically and quantitatively, but is also assessed qualitatively. When heterogeneity is too large, meta-analysis generally does not yield meaningful results.</p><p>There are two general categories of heterogeneity: clinical and statistical.</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Clinical heterogeneity</strong> refers to basic differences in trial design <span class=\"nowrap\">and/or</span> participants across studies. Such heterogeneity may include differences in specific drugs (eg, different statins), drug dosages, duration of treatment, surgical technique, patient comorbidities, or other characteristics. As a general rule, sets of studies with large clinical heterogeneity (a concept that can be difficult to define precisely) should not be mathematically combined (meta-analyzed).</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Statistical heterogeneity</strong> refers to variability in the range of results across studies. Statistical heterogeneity may be due to either chance or underlying clinical heterogeneity. There are several tests to measure the degree of statistical heterogeneity. Studies are described as being homogeneous when no statistical heterogeneity is detected.</p><p/><p class=\"headingAnchor\" id=\"H2326452\"><span class=\"h1\">OVERVIEW OF SYSTEMATIC REVIEW AND META-ANALYSIS</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Several steps are essential for conducting a systematic review or meta-analysis. These include:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Developing a protocol</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Formulating research questions</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Searching for the evidence</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Assessing the quality of studies</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Displaying results</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Exploring reasons for the heterogeneity of results across studies</p><p/><p>The basic steps, along with limitations that should be considered, are discussed here. While this topic review focuses on meta-analysis of randomized controlled trials, many of the methods and issues apply equally to meta-analyses of other comparative studies, noncomparative and other observational studies, and studies of diagnostic tests. An overview of approaches to systematic review and meta-analysis is provided in a table (<a href=\"image.htm?imageKey=PC%2F80657\" class=\"graphic graphic_table graphicRef80657 \">table 1</a>).</p><p class=\"headingAnchor\" id=\"H2327460\"><span class=\"h2\">Consensus statements</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The 2009 Preferred Reporting Items of Systematic reviews and Meta-Analyses (PRISMA) <a href=\"http://www.prisma-statement.org/PRISMAStatement/PRISMAStatement.aspx&amp;token=mGCp3jf+U+RF8NnZx3snHv23JvyZHFuw2BQvvMqH/TjTMtv75T4gxC+cKZMfaCgExPonCgH29dtCpZpCzjPV8g==&amp;TOPIC_ID=16293\" target=\"_blank\" class=\"external\">statement</a> emphasizes that systematic reviews should provide the protocol, data, and assessments of risk of bias from individual studies in sufficient detail to allow the reader to verify the results [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/10\" class=\"abstract_t\">10</a>]. It underscores the basic questions that the clinician and investigator should ask when interpreting a systematic review. Several &quot;<a href=\"http://www.prisma-statement.org/Extensions/Default.aspx&amp;token=i20xYI8aSBFiAGc5wtYRs651Bna3iA4Qk4trdocUrr+98tPGhICW5bd+x4knFhqLMwhc4JBEU3LrKG2dfTXb4g==&amp;TOPIC_ID=16293\" target=\"_blank\" class=\"external\">extensions</a>&quot; to PRISMA have been developed for specific types of systematic reviews or meta-analyses (eg, harms, network meta-analyses) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/11\" class=\"abstract_t\">11</a>]. In addition, readers of systematic reviews should assess the relevance to their own practice in regard to the studied populations, settings, interventions, and outcomes assessed.</p><p>The Institute of Medicine has published recommended standards for developing systematic reviews [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/12\" class=\"abstract_t\">12</a>]. While these standards principally apply to publicly-funded systematic reviews of comparative effectiveness research that focus specifically on treatments, most of the standards pertain to all systematic reviews.</p><p class=\"headingAnchor\" id=\"H2326459\"><span class=\"h1\">DEVELOPING A PROTOCOL</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A written protocol serves to minimize bias and to ensure that the review is implemented according to reproducible steps. A systematic review should describe the research questions and the review methodology, including the search strategy and approach to analyzing the data. Ideally, the protocol should be a collaborative effort that includes both clinical and methodology experts [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/13\" class=\"abstract_t\">13</a>]. </p><p>Publication of protocols can be useful to prevent unnecessary duplication of efforts and to enhance transparency of the systematic review. A voluntary registry, <a href=\"http://www.crd.york.ac.uk/PROSPERO/&amp;token=8WmZyxtCPDhnIAxHBR5GfGvJ6Q1klmyn3WHXY07Oqvwj31GdzXJLenhYYw7R1wi6&amp;TOPIC_ID=16293\" target=\"_blank\" class=\"external\">PROSPERO</a>, was established in 2011.</p><p class=\"headingAnchor\" id=\"H2326466\"><span class=\"h2\">Formulating research questions</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Research questions (often referred to as &quot;key questions&quot;) are analogous to the research hypotheses of primary research studies. They should be focused and defined clearly, since they determine the scope of research the systematic review will address [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/14\" class=\"abstract_t\">14</a>].</p><p>Broad questions that cover a range of topics may not be directly answerable and are not appropriate for systematic reviews or meta-analyses. As an example, the question &quot;What is the best treatment for chronic hepatitis B?&quot; would need to be broken down into several smaller well-focused questions that could be addressed in individual and complementary systematic reviews. Examples of appropriate key questions may include, &quot;How does <a href=\"topic.htm?path=entecavir-drug-information\" class=\"drug drug_general\">entecavir</a> compare with placebo for achieving HBeAg seroconversion in patients with chronic HBeAg-positive hepatitis B?&quot; and &quot;What is the relationship between hepatitis B genotypes and response rates to entecavir?&quot; These and other related questions would be addressed individually and then, ideally, considered together to answer the more general question.</p><p>Key questions for studies of the effectiveness of interventions are commonly formulated according to the &quot;PICO&quot; method, which fully defines the <strong>P</strong>opulation, the <strong>I</strong>ntervention, the appropriate <strong>C</strong>ontrol or <strong>C</strong>omparator, and the <strong>O</strong>utcomes of interest [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/14\" class=\"abstract_t\">14</a>]. Investigators must also decide which study designs (eg, only randomized trials) are appropriate to include, along with other eligibility criteria. Variations of these criteria should be used for systematic reviews of other study designs, such as of cohort studies (without a comparator), studies of exposures (eg, mercury intake), or studies of diagnostic tests.</p><p class=\"headingAnchor\" id=\"H2326473\"><span class=\"h1\">PERFORMING THE LITERATURE SEARCH</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The literature search provides the evidence for the review and should be systematic and comprehensive to minimize error and bias [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/14\" class=\"abstract_t\">14</a>]. Most meta-analyses start with a search of an electronic database of the literature. PubMed [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/15\" class=\"abstract_t\">15</a>] is almost universally used, but many systematic reviews also include searches in Embase [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/16\" class=\"abstract_t\">16</a>] and the Cochrane Central Register of Controlled Trials (CENTRAL) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/17\" class=\"abstract_t\">17</a>]. Inclusion of additional databases should be considered for specialized topics such as complementary or alternative medicine, quality of care, or nursing. Electronic searches should be supplemented by searches of the bibliographies of retrieved articles and relevant review articles, and by studies known to domain experts.</p><p>The research community has also recognized a need to incorporate the &quot;grey literature&quot; to diminish the risks of publication bias (selective publication of studies, possibly based on their results) and reporting bias (selective reporting of study results, possibly based on statistical significance) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/12,18-20\" class=\"abstract_t\">12,18-20</a>]. There is no standard definition of grey literature, but it generally refers to information obtained from sources other than published, peer-reviewed articles, such as conference proceedings, clinical trial registries, adverse events databases, government agency databases (eg, US Food and Drug Administration) and documents, unpublished industry data, dissertations, and online sites.</p><p class=\"headingAnchor\" id=\"H2326480\"><span class=\"h2\">Publication bias</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A potentially important limitation of systematic review is &quot;publication bias,&quot; which can result from the fact that not all research is published. Compared with positive studies, negative studies may be less likely to be published and more likely to take longer to be published [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/21\" class=\"abstract_t\">21</a>], which can affect the validity of systematic reviews [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/21\" class=\"abstract_t\">21</a>], which can affect the validity of systematic reviews [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/22\" class=\"abstract_t\">22</a>]. Similarly, outcome reporting bias relates to the concern that non-significant or non-favorable outcomes are selectively not reported in publications. Many other potential sources of bias have been described [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/23\" class=\"abstract_t\">23</a>] and included in consensus standards for developing systematic reviews [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/12\" class=\"abstract_t\">12</a>].</p><p>Several methods have been developed to evaluate whether publication bias is present. However, they all require making major assumptions about possible missing studies and are controversial regarding their validity [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/24\" class=\"abstract_t\">24</a>]. Any evaluation of publication bias should not be considered definitive, but rather only exploratory in nature. Methods to evaluate for publication bias include:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>The funnel plot &ndash; This commonly used method is a scatter plot that displays the relationship between the weight of the study (eg, study size or standard error) and the observed effect (<a href=\"image.htm?imageKey=PC%2F65012\" class=\"graphic graphic_figure graphicRef65012 \">figure 1</a>) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/25\" class=\"abstract_t\">25</a>]. In principle, larger studies should display less variability of the treatment effects. Asymmetric appearance, especially due to the absence of smaller negative studies, can suggest unpublished data. Several statistical methods have been developed to analyze funnel plots. However, the validity of funnel plots has been called into question because they lack a statistical foundation, their interpretation is subjective, and asymmetry could be due to factors other than unpublished negative studies (such as population heterogeneity or study quality) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/22,26-28\" class=\"abstract_t\">22,26-28</a>].</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>The &quot;trim and fill&quot; method &ndash; This approach determines if adding hypothetical studies to an asymmetrical funnel plot (to make it symmetrical) would change the effect size estimate of the meta-analysis results [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/29\" class=\"abstract_t\">29</a>].</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>The &quot;modeling selection process&quot; &ndash; This uses a model to determine how effect size estimates would differ with and without a selection process (ie, publication bias) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/30\" class=\"abstract_t\">30</a>]. Several methods have been proposed for developing these models. The &quot;fail-safe N&quot; determines the number of studies with an effect size of zero that would be needed to make the meta-analysis effect size non-significant [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/31\" class=\"abstract_t\">31</a>].</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>A method has been developed to test for an excess of studies with significant results [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/32\" class=\"abstract_t\">32</a>].</p><p/><p class=\"headingAnchor\" id=\"H2326487\"><span class=\"h1\">ASSESSING THE QUALITY OF STUDIES</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The quality of an individual study has been defined as the &quot;confidence that the trial design, conduct, and analysis has minimized or avoided biases in its treatment comparison&quot; [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/33\" class=\"abstract_t\">33</a>]. Quality assessment represents the extent to which trial design and methodology prevented systematic error, and can help explain differences in the results of systematic reviews. The process of assessing study quality is not straightforward. Many approaches have been proposed that include assessment of certain indicators of study quality as well as formal quality scoring systems that combine quality indicators. The assessment of study quality is limited by the need to rely on information presented in the manuscript and an understanding that there is no true reference standard for quality [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/34\" class=\"abstract_t\">34</a>]. The term &quot;risk of bias&quot; is being used with increased frequency to focus more on problems with study design and other specific biases, as distinct from reporting errors and other non-bias-related factors.</p><p>Critical appraisal of individual studies should be performed as part of a meta-analysis, but the results should be interpreted cautiously. The primary value of such appraisal may be to determine the degree of confidence that the estimate of effect reflects the &quot;truth&quot; as best as it can be measured. Differences in the quality or risk of bias of individual studies can also be explored to help explain heterogeneity.</p><p>Commonly used quality indicators include:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Adequate randomization:</strong> Whether randomization was adequately performed (eg, by a computer algorithm versus an alphabetical list that may be subject to bias).</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Allocation concealment:</strong> Whether the assignment of the treatment group was adequately concealed from the investigators. For example, a study may be biased if the investigators used unsealed envelopes to assign patients to one group or another. The investigators could read the contents and thereby channel certain patients into desired treatment groups.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Dropout rates and reporting:</strong> High rates of withdrawal of participants from a study may indicate a fundamental problem with the study design. Uneven withdrawal from different study groups can lead to bias, particularly if the reasons for withdrawal differ between interventions. Reports should describe the reasons for patient withdrawal to allow assessment of their effect on bias and study applicability.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Reporting accuracy:</strong> The accuracy of reporting of study methodology, patient characteristics, and study results. Inconsistent or incomplete reporting, missing data on important patient characteristics, and incomplete analyses of outcomes diminish confidence in a study's findings.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Statistical interpretations: </strong>The appropriateness of statistical analyses.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Blinding:</strong> Adequate masking of study participants, study researchers, caregivers, outcome assessors, and data assessors to group assignment. Blinding is not always feasible. Some forms of surgery or behavioral modifications, for example, do not lend themselves to blinding. &quot;Double blinding&quot; generally refers to blinding of the study participants and at least one of the study investigators. For adequate blinding, treatments with a noticeable side effect (eg, <a href=\"topic.htm?path=vitamin-b3-niacin-drug-information\" class=\"drug drug_general\">niacin</a>) should have an &quot;active control&quot; that mimics the side effect.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Similarity between groups: </strong>Similarity of groups being compared at baseline, in concomitant treatment, and in follow-up.</p><p/><p>In theory, studies in which these measures are poorly achieved are more likely to result in biased outcomes. For example, compared with double-blinded trials, non-blinded studies may exaggerate the benefit of treatment (eg, a larger odds ratio, or treatment effect) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/35\" class=\"abstract_t\">35</a>]. However, evaluation of studies using different quality scales can result in paradoxical conclusions, such that designation of a study as &quot;high quality&quot; using one scale may not be corroborated using a different scale [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/36\" class=\"abstract_t\">36</a>].</p><p>Only inadequate allocation concealment or lack of double blinding have been shown to be related to exaggerated estimates of treatment effects, although with limited empirical evidence [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/37\" class=\"abstract_t\">37</a>]. However, even these measures have been questioned for their influence on effect sizes [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/38-40\" class=\"abstract_t\">38-40</a>]. Furthermore, evaluations of the relationship between various individual quality factors (including randomization concealment and double blinding) and effect sizes in randomized controlled trials of interventions found that individual quality measures were not consistently associated with the magnitude of treatment effect across studies, clinical areas, or outcome types [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/41,42\" class=\"abstract_t\">41,42</a>].</p><p class=\"headingAnchor\" id=\"H96519889\"><span class=\"h1\">FOREST PLOTS</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A forest plot is a graphical presentation of individual studies, typically displayed as point estimates with their associated 95% confidence intervals on an appropriate scale, next to a description of the individual studies. The forest plot allows the reader to see the estimate and the precision of the individual studies, appreciate the heterogeneity of results, and compare the estimates of the individual studies to the overall summary estimate.</p><p>An example of the meta-analysis of the net change in glucose concentrations with chromium supplementation is shown in a figure (<a href=\"image.htm?imageKey=PC%2F59845\" class=\"graphic graphic_figure graphicRef59845 \">figure 2</a>) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/43\" class=\"abstract_t\">43</a>]. Ideally, a forest plot should provide sufficient data for the reader to make some assessment of the individual studies in the context of the overall summary (eg, to compare sample sizes, baseline values, demographic features, and study quality).</p><p class=\"headingAnchor\" id=\"H110188885\"><span class=\"h1\">META-ANALYSIS</span></p><p class=\"headingAnchor\" id=\"H2326494\"><span class=\"h2\">Statistical methods for combining data</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Basic meta-analysis combines results across studies to provide overall estimates and confidence intervals of treatment effects. Clinical trials often report dichotomous outcome data (outcomes with two possible states, such as death versus survival), and their results are often expressed as odds ratios, relative risks (risk ratios), risk differences, or number needed to treat (NNT). Essentially any study metric can be meta-analyzed, including continuous variable means, changes, or differences; proportions; or hazard ratios.</p><p>For meta-analysis, odds ratio or relative risk, which measure the relative effect, are preferable to metrics that measure absolute differences (risk difference and NNT) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/44,45\" class=\"abstract_t\">44,45</a>]. In meta-analyses, risk differences (and thus NNT) are more likely to be heterogeneous than relative risk or odds ratio due to varying baseline risks across studies. As a result, combining risk differences is frequently problematic [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/46-48\" class=\"abstract_t\">46-48</a>]. One method that helps overcome this concern is to perform a meta-analysis of the relative risk, then apply that estimate to a reasonable range of control rates (outcome rates in untreated groups). This permits calculation of a range of risk differences. (See <a href=\"topic.htm?path=glossary-of-common-biostatistical-and-epidemiological-terms#H13\" class=\"medical medical_review\">&quot;Glossary of common biostatistical and epidemiological terms&quot;, section on 'Number needed to treat'</a>.)</p><p>The summary effect estimate can be calculated under the assumption of a &quot;fixed effect&quot; or a &quot;random effects&quot; model [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/49\" class=\"abstract_t\">49</a>].</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Fixed effect model: </strong>A hypothetical model for a fixed effect model meta-analysis is shown in a figure (<a href=\"image.htm?imageKey=PC%2F80325\" class=\"graphic graphic_figure graphicRef80325 \">figure 3</a>). The model assumes there is a single true treatment effect and that all trials provide estimates of this one true effect. The meta-analysis thus provides a pooled estimate of the single true effect.</p><p/><p class=\"bulletIndent1\">The central assumption of a fixed effect model is that estimates from each study differ solely because of random error around a common true effect. This assumes that all studies represent the same population, intervention, comparator, and outcome for which there is a single &quot;true&quot; effect size. Fixed effects models yield effect size estimates by assigning a weight to each individual study estimate that reflects the inherent variability in the results measured (ie, the &quot;within-study variance&quot; related to the standard error of the outcome).</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span><strong>Random effects model: </strong>In contrast, the central assumption of random effects models is that each study estimate represents a random sample from a distribution of different populations [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/50\" class=\"abstract_t\">50</a>]. A hypothetical model for a random effects model meta-analysis is shown in a figure (<a href=\"image.htm?imageKey=PC%2F51482\" class=\"graphic graphic_figure graphicRef51482 \">figure 4</a>). The model assumes there are multiple true treatment effects related to inherent differences in different populations or other factors, and that each trial provides an estimate of its own true effect. The meta-analysis thus provides a pooled estimate across (or an average of) a range of true effects.</p><p/><p class=\"bulletIndent1\">The random effects model assumes that there is not necessarily one &quot;true&quot; effect size but rather that the studies included have provided a glimpse of a range of &quot;true&quot; effects. The random effects model incorporates both &quot;between-study variance&quot; (to capture the range of difference effects across studies) and &quot;within-study variance&quot; (to capture the range of difference effects within studies) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/49\" class=\"abstract_t\">49</a>].</p><p/><p>The choice of methods is important because of the implications for interpreting the results. When heterogeneity is present, the confidence intervals of estimates derived using the random effects model are wider than those from the fixed effect model. Thus the random effects model, generally, provides a more &quot;conservative&quot; estimate (eg, it is less likely to find a falsely significant effect). As a result, most experts use random effects models. However, even the random effects model may not be sufficiently conservative, especially in areas where there are small numbers of heterogeneous studies. Some experts recommend that alternative methods may be preferred in these settings [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/51\" class=\"abstract_t\">51</a>].</p><p>There are limited instances when it is appropriate to use a fixed effects model for summarizing clinical trials. These include meta-analyses in which:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>There is extreme confidence that the studies are comparable (ie, characteristics of the enrolled patients, the type of intervention, comparators and outcome measures) such that any difference across studies is just due to random variation. Such an assumption is typically difficult to justify. One example of an appropriate use of the fixed effects model is meta-analysis of repeated, identical, highly-controlled trials in a uniform setting, as may be done by pharmaceutical companies during early testing.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>The studies are of rare events in which one form of a fixed effects model (the Peto odds ratio) may be less biased than other methods of pooling data [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/52\" class=\"abstract_t\">52</a>].</p><p/><p class=\"headingAnchor\" id=\"H2326501\"><span class=\"h2\">When to combine studies</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The decision to combine studies should be based upon both qualitative and quantitative evaluations. Important qualitative features include the degree of similarity of populations, interventions, outcomes, study objectives, and study designs that incorporate both clinical and biological plausibility. The systematic reviewers should provide a sufficient explanation of the rationale for combining studies to allow the readers to judge for themselves whether they agree that it was appropriate to combine the individual studies.</p><p>Quantitative methods to examine heterogeneity also should be considered in making the decision or determining if it is appropriate to combine data. These typically involve the I<sup>2 </sup>index or Q statistic. These statistics, however, generally have low power and are thus prone to false negative results (eg, not detecting heterogeneity when it is present).</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>The I<sup>2</sup> index represents the amount of variability in the effect sizes across studies that can be explained by between-study variability. For example, an I<sup>2</sup> value of 75 percent means that 75 percent of the variability in the measured effect sizes across studies is caused by true heterogeneity among studies. By consensus, standard thresholds for the interpretation of I<sup>2</sup> are 25, 50, and 75 percent to represent low, medium, and high heterogeneity, respectively [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/53\" class=\"abstract_t\">53</a>]. However, the investigators who introduced the I<sup>2</sup> statistic noted that na&iuml;ve categorization of I<sup>2</sup> values is not appropriate in all circumstances and that &quot;the practical impact of heterogeneity in a meta-analysis also depends on the size and direction of treatment effects&quot; [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/53\" class=\"abstract_t\">53</a>]. The clinical implication and interpretability of a meta-analysis with a large I<sup>2</sup> index will be different for studies with large statistically significant effects compared with studies with smaller inconsistent effects.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>The &quot;Q&quot; statistic tests the hypothesis that results across studies are homogeneous. The Q statistic involves performing a chi square test for heterogeneity (eg, summing the squared deviations from the effect measured in each study from the overall effect and weighting the contribution from each study by the inverse of its variance). A nonsignificant value suggests that the studies are homogeneous. However, the Q statistic has limited power to detect heterogeneity in meta-analyses with few studies, while it tends to over-detect heterogeneity in meta-analyses with many studies [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/54\" class=\"abstract_t\">54</a>]. Commonly, the Q statistic is interpreted to indicate heterogeneity if its P value is less than 0.10.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Other heterogeneity measures (eg, H<sup>2</sup> , R<sup>2</sup>, tau<sup>2</sup>) have also been described but are rarely used.</p><p/><p class=\"headingAnchor\" id=\"H7125075\"><span class=\"h2\">Precision</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Precision refers to the extent to which the observed results would be reproduced exactly given the same interventions and study design. Precision can be contrasted with validity, which refers to the extent to which the results reflect the &quot;truth&quot; (<a href=\"image.htm?imageKey=PC%2F52219\" class=\"graphic graphic_figure graphicRef52219 \">figure 5</a>).</p><p>An important advantage of meta-analysis is that combining studies produces more precise estimates of the effect size (ie, estimates that have smaller confidence intervals) due to the increased sample size. However, the increased precision can overshadow important differences in the included studies. The forest plot and the quantitative measures of heterogeneity can help uncover whether such differences exist and lead to further analysis to understand the reasons for the differences. As an example, a researcher interested in understanding the prevalence of hepatitis B could combine rates in various countries to derive an overall estimate. The estimate would be precise (ie, narrow confidence intervals) because it would include a large amount of data (ie, from the entire world). However, it is well known that the prevalence of hepatitis B varies greatly in different geographic regions. An analysis stratified by geography, while offering less precise estimates, would provide a more realistic (valid) assessment of the true prevalence in various regions.</p><p>When interpreting a meta-analysis, an important clinical question is whether the pooled estimated is sufficiently precise to permit a confident conclusion about the results. Imprecision should be suspected when there is a small sample size (and particularly a small number of events) and when the confidence intervals are wide (leaving uncertainty about the magnitude of the true effect). The question is how wide should be considered too wide?</p><p>As a general rule, results are not sufficiently precise to generate high confidence in their interpretation if the clinical decision based on the result (eg, to use or not use an intervention) would change at the outer boundaries of the 95% CI for the summary estimate. For organizations issuing guidelines, the strength of the evidence should be downgraded for imprecision. Specific criteria for imprecision have been developed in the context of grading for guidelines [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/55\" class=\"abstract_t\">55</a>].</p><p>When an effect is suggested, but imprecision is present, more studies (or in some cases, analysis of specific subgroups of studies) may be needed to improve the precision.</p><p class=\"headingAnchor\" id=\"H2327699\"><span class=\"h2\">Sensitivity analysis</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A meta-analysis should test how stable the overall estimates are when different subgroups of studies are analyzed and should explore heterogeneity among the studies. Meta-regression and subgroup analyses can be used to examine the influence on the overall results. When feasible, meta-analysis of individual patient data allows the most rigorous exploration of heterogeneity. (See <a href=\"#H2326516\" class=\"local\">'Individual patient data'</a> below.)</p><p>Explorations such as reanalyzing the data with single studies or groups of studies (eg, &quot;poor quality&quot; studies) omitted can be used to determine the degree to which overall results are being driven by these studies. Conclusions should seldom be driven by a single study, since the meta-analysis would add little additional information or confidence compared with the single study alone.</p><p>Sensitivity analyses can also be used to explore such issues as publication or reporting bias. As an example, finding that meta-analysis of the largest studies yields smaller effect sizes than meta-analysis of all trials can suggest that smaller &quot;negative&quot; trials may be missing [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/56,57\" class=\"abstract_t\">56,57</a>].</p><p class=\"headingAnchor\" id=\"H2327476\"><span class=\"h2\">Meta-regression</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Regression analysis of primary studies may be used to adjust for potential confounders or explain differences in results among subjects. This meta-analytic technique is commonly known as meta-regression. In this approach, the dependent variable in the regression is the estimate of treatment effect from each individual study, and the independent variables (eg, covariates such as drug dosage, treatment duration, or study size) are the aggregated characteristics derived from the individual studies. Instead of individual patients serving as the units of analysis, each individual study is considered to be one observation [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/58-60\" class=\"abstract_t\">58-60</a>]. Meta-regression tests the statistical interaction between the subgroup variable (eg, dose) and the treatment effect (eg, relative risk of death). It can include categorical variables (including two or more categories, such as study country or study design) and continuous variables (such as dose or follow-up duration) either singly (univariable analysis) or together (multivariable analysis). An example of a meta-regression of early trials of <a href=\"topic.htm?path=zidovudine-drug-information\" class=\"drug drug_general\">zidovudine</a> monotherapy for HIV infection is shown in a figure (<a href=\"image.htm?imageKey=PC%2F69060\" class=\"graphic graphic_figure graphicRef69060 \">figure 6</a>) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/61\" class=\"abstract_t\">61</a>]. The meta-regression successfully explains the heterogeneity across studies, showing an association between treatment duration and the effect of treatment on death that was not apparent within the individual trials.</p><p>There are several caveats related to the performance and interpretation of meta-regression:</p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Meta-regression and subgroup analyses (that rely on retrospective data from previously run trials) should be considered to yield hypothesis-generating, rather than conclusive, associations, in contrast to well-designed regressions of prospective study data.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Meta-regression is not always feasible since covariates may not be fully reported or may not be uniformly defined.</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Data dredging (analyzing every possible variable regardless of clinical relevance) can result in spurious associations [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/62\" class=\"abstract_t\">62</a>].</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>It may be difficult to account properly for patient-level variables (such as age, sex, or laboratory values). Most studies, for example, report averages for such variables (eg, a mean age of 47.1 years) that do not reflect the range of values across the study population. Making an assumption about individual data based upon aggregated statistics (known as &quot;ecological fallacy&quot;) can produce invalid results in meta-regression [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/63\" class=\"abstract_t\">63</a>].</p><p/><p>Two hypothetical examples of studies susceptible to ecological fallacy are presented in a figure (<a href=\"image.htm?imageKey=PC%2F63627\" class=\"graphic graphic_figure graphicRef63627 \">figure 7</a>). The only reliable way to determine the presence of ecological fallacy is to evaluate whether analyses of patient-level data (eg, within-study data) are consistent with analyses across studies (eg, meta-regression).</p><p class=\"headingAnchor\" id=\"H2327483\"><span class=\"h2\">Subgroup analyses</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A simpler alternative to meta-regression is to categorize studies into subgroups based upon clinically relevant variables. Separate meta-analyses of these subgroups can then be graphically compared. Subgroup analysis is subject to the same limitations inherent to meta-regression, including risks associated with data dredging and ecological fallacy. Graphical comparisons and meta-regression can be useful together since the former provides an overview while the latter permits multivariable analysis and evaluation of interactions. In a systematic review of treatment of obstructive sleep apnea on depressive symptoms, a subgroup analysis shows details about the few studies of patients with baseline depression and the more numerous studies of patients with no baseline depression (<a href=\"image.htm?imageKey=PC%2F104212\" class=\"graphic graphic_figure graphicRef104212 \">figure 8</a>) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/64\" class=\"abstract_t\">64</a>]. A meta-regression from the same systematic review allows statistical comparisons between the effects in the different subgroups, in this case finding that p&lt;0.001 for the comparison of effects in studies of people with baseline depression versus studies of people without baseline depression (<a href=\"image.htm?imageKey=PC%2F104213\" class=\"graphic graphic_table graphicRef104213 \">table 2</a>).</p><p class=\"headingAnchor\" id=\"H2326516\"><span class=\"h2\">Individual patient data</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>It is sometimes possible to obtain original patient-level databases in order to reanalyze individual patient data in a meta-analysis [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/13\" class=\"abstract_t\">13</a>]. Pooling individual patient data is considered to be the most rigorous form of meta-analysis. While more costly and time consuming, and limited by difficulties collecting original data, there are a number of benefits. These include the ability to perform meta-regressions of patient-level predictors (eg, age) without the risk of ecological fallacy, time-to-event analyses, and to include unpublished, previously unanalyzed data. However, analyses of partial databases (all that may be available with proprietary data) or of selected databases are subject to selection bias or limited generalizability of results, similar to other retrospective analyses of incomplete samples.</p><p class=\"headingAnchor\" id=\"H2326523\"><span class=\"h2\">Network meta-analysis</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Systematic reviews commonly compare the relative effects of several interventions (eg, different statins), not just the comparison of two specific interventions. However, individual trials generally compare selected interventions to each other (eg, one statin to another). When multiple specific interventions are compared across trials, a network of studies can be established where all the studied interventions are linked to each other by individual trials. Network meta-analysis (also termed &quot;mixed treatment comparison&quot; or &quot;multiple treatment comparison&quot;) evaluates all studies and all interventions simultaneously to produce multiple pairwise estimates of relative effects of each intervention compared with every other intervention [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/65,66\" class=\"abstract_t\">65,66</a>].</p><p>A diagram (or topology) of a network meta-analysis of trials of all antihypertensive drugs and the risk of cancer is shown in a figure (<a href=\"image.htm?imageKey=PC%2F79458\" class=\"graphic graphic_figure graphicRef79458 \">figure 9</a>) [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/67\" class=\"abstract_t\">67</a>]. Although no trials directly compared combination of angiotensin converting enzyme inhibitors (ACEi) and angiotensin receptor blockers (ARBs) with placebo, an indirect comparison can be made by analyzing all the studies that compared ACEi and ARBs with either ACEi alone or ARBs alone and that compared ACEi or ARBs with placebo. Network meta-analyses with sufficient data should compare the direct comparisons (eg, ACEi versus ARBs) with the indirect comparisons (eg, the same comparison derived from trials of ACEi versus placebo and ARBs versus placebo).</p><p>Greater care is required to ensure that a network meta-analysis is appropriate and valid than may be needed for a simple meta-analysis [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/66\" class=\"abstract_t\">66</a>]. The benefits of randomization are lost when making comparisons across trials. Thus, it is particularly important that studies included in a network meta-analysis are similar in their participants, setting, ancillary treatments, and other relevant parameters.</p><p>If studies of 'X' versus placebo are systematically different than studies of 'Y' versus placebo, then an indirect comparison of 'X' versus 'Y' may be biased due to the differences between studies of 'X' and 'Y'. As an example, if studies of 'X' versus placebo were conducted decades ago and studies of 'Y' versus placebo are more recent, the differences in the indirect comparison of 'X' versus 'Y' may be partly due to differences in disease management over the intervening decades. One method of testing the validity of a network is to compare direct and indirect comparisons of the same pairs of interventions. Networks in which direct and indirect comparisons disagree are labeled as inconsistent. Those with substantial inconsistency should be analyzed carefully or not at all. Attempts should be made to explain the inconsistency.</p><p>Bayesian methods are commonly used to conduct network meta-analysis [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/68\" class=\"abstract_t\">68</a>]. This approach has the advantage of allowing estimation of the probability of each intervention being best, which, in turn, allows interventions to be ranked. Such ranking, however, needs to be interpreted cautiously, as it can be unstable, depending on the network topology, and can have a substantial degree of imprecision [<a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/69\" class=\"abstract_t\">69</a>].</p><p class=\"headingAnchor\" id=\"H2326530\"><span class=\"h1\">READING AND INTERPRETING A SYSTEMATIC REVIEW</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Many of the steps involved in conducting a systematic review with meta-analysis can be difficult to understand without appropriate training. In some cases, acceptance of the assumptions made by the reviewers (eg, why certain studies were excluded, why particular outcomes were considered to be clinically similar or distinct) require a degree of faith since they are not always obvious from reading the manuscript. The temptation for readers is to accept the results on faith that the authors and a journal's peer-review process have produced a valid summary of the body of evidence. However, journal readers should appraise systematic reviews, like all primary studies, for their quality and the extent to which the findings are applicable to the questions being posed.</p><p>Systematic review and meta-analysis are subject to the same biases observed in all retrospective analyses. Researchers' focus or biases (due to factors such as funding source, researchers' agendas or specialties) may subtly affect systematic reviews just as they may affect the eligibility of subjects or protocols in an individual study. In addition, the value of a systematic review's conclusions may be limited by the quality and applicability of the underlying studies.</p><p>Since meta-analysis is a pooling of distinct individual studies, it is important to bear in mind that the overall results, even more than individual study results, are not directly interpretable as a patient-level risk (of an outcome) and cannot make personalized predictions for patients. Results must be interpreted as an average result for a population.</p><p>These potential biases should not discredit the process of systematic review, but instead readers should apply an equally critical eye to systematic reviews as to primary studies. Questions to consider when evaluating a systematic review are presented in a table (<a href=\"image.htm?imageKey=PC%2F56270\" class=\"graphic graphic_table graphicRef56270 \">table 3</a>).</p><p class=\"headingAnchor\" id=\"H2326537\"><span class=\"h1\">GLOSSARY OF TERMS</span></p><p class=\"headingAnchor\" id=\"H110188417\"><span class=\"h2\">Applicability (generalizability)</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>The relevance of a study (or a group of studies) to a population of interest (or an individual patient). This requires an assessment of how similar the subjects of a study are to the population of interest, the relevance of the studied interventions and outcomes, and other PICO features. (See <a href=\"#H110188348\" class=\"local\">'PICO method (PICOD, PICOS, PICOTS, others)'</a> below.)</p><p class=\"headingAnchor\" id=\"H2327798\"><span class=\"h2\">Ecological fallacy (ecological inference fallacy)</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>An error in interpreting data where inferences are made about specific individuals based upon aggregated statistics for groups of individuals.</p><p class=\"headingAnchor\" id=\"H110188273\"><span class=\"h2\">Fixed effect model</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A statistical method for combining data that incorporates only within-study variance. Assumes that all studies represent the same population for which there is a single &quot;true&quot; effect size.</p><p class=\"headingAnchor\" id=\"H110188336\"><span class=\"h2\">Forest plot</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A graphical display of the results, with 95% confidence intervals, of the individual studies that go into a meta-analysis together with the meta-analytic summary effect size (and confidence interval). Said to look like a forest of trees.</p><p class=\"headingAnchor\" id=\"H110188387\"><span class=\"h2\">Funnel plot</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A graphical technique, with related statistical tests, to examine the studies within a systematic review for the possibility of publication bias.</p><p class=\"headingAnchor\" id=\"H110188399\"><span class=\"h2\">Grey literature</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A term with varying and shifting meaning that indicates sources of evidence beyond the peer-reviewed, published literature available in major databases (eg, Medline). Examples include alternative databases, conference abstracts and proceedings, unpublished studies (eg, via clinicaltrials.gov), newspaper or internet citations, citation indexes, handsearching of journals or reference lists, and domain experts.</p><p class=\"headingAnchor\" id=\"H110188136\"><span class=\"h2\">Heterogeneity (statistical heterogeneity)</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Differences in study results across studies examining similar questions. Statistical heterogeneity may be due to differences in specific features of study design or to chance. Measured with a variety of statistical tests, most commonly I<sup>2</sup> and the Q statistic.</p><p class=\"headingAnchor\" id=\"H110188185\"><span class=\"h3\">Clinical heterogeneity</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Differences in study features, such as study eligibility criteria, interventions, or methods of measuring outcomes, that may preclude appropriate meta-analysis. These features can be explicit (such as different drug doses used) or implicit (such as differences in populations depending on setting or country). Clinical heterogeneity may or may not result in statistical heterogeneity, but often may not, for example if the effect size is similar regardless of the drug dose, of the individual drug within a class of drugs, or in different populations (eg, men and women, or Japanese and Americans). (See <a href=\"#H110188136\" class=\"local\">'Heterogeneity (statistical heterogeneity)'</a> above.)</p><p class=\"headingAnchor\" id=\"H110188287\"><span class=\"h2\">I2 index</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A statistical test to describe heterogeneity among studies. It can be interpreted as the percentage of the total variability that is due to true heterogeneity (as opposed to chance). Values of 25, 50, and 75 percent are commonly considered to represent low, medium, and high heterogeneity, respectively.</p><p class=\"headingAnchor\" id=\"H110188249\"><span class=\"h2\">Key questions</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Research questions that are clearly defined and form the basis for the systematic review or meta-analysis.</p><p class=\"headingAnchor\" id=\"H110188324\"><span class=\"h2\">Meta-regression</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A meta-analytic technique that permits adjustment for potential confounders and analysis of different variables to help explain differences in results across studies. Equivalent to patient-level regression, except that the unit of analysis is a study instead of a person.</p><p class=\"headingAnchor\" id=\"H110188312\"><span class=\"h2\">Network meta-analysis</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A technique to simultaneously meta-analyze a network of studies that evaluated related, but different, specific comparisons. It permits quantitative inferences across studies that have made indirect comparisons of interventions. An example would be the comparison of two or more drugs to each other, when each was studied in comparison to placebo.</p><p class=\"headingAnchor\" id=\"H110188348\"><span class=\"h2\">PICO method (PICOD, PICOS, PICOTS, others)</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>An acronym that stands for <strong>P</strong>opulation, <strong>I</strong>ntervention(s), <strong>C</strong>omparator(s), <strong>O</strong>utcome(s); added letters include Study Design (PICOD), Setting (PICOS), Timing and Setting (PICOTS). PICO is the basis for a systematic approach in developing a key question and research protocol. While used extensively for systematic reviews, PICO is relevant to all medical research questions. Each feature is defined explicitly and comprehensively so that it is unambiguously evident which studies are eligible for inclusion in a systematic review.</p><p class=\"headingAnchor\" id=\"H110189133\"><span class=\"h2\">Precision</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Precision refers to the extent to which the observed results would be reproduced exactly, given the same interventions and study design. Precision can be contrasted with validity, which refers to the extent to which the results reflect the &quot;truth.&quot;</p><p class=\"headingAnchor\" id=\"H110188452\"><span class=\"h2\">PRISMA statement</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>Preferred Reporting Items of Systematic reviews and Meta-Analyses, an update of QUOROM (Quality of Reporting of Meta-analyses) statement. A guideline for reporting of systematic reviews, used as a standard by many journals.</p><p class=\"headingAnchor\" id=\"H587111879\"><span class=\"h2\">PROSPERO</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>An international database of prospectively registered systematic reviews in health care published since 2011. PROSPERO creates a permanent record of systematic review protocols to reduce unnecessary duplication of efforts and increase transparency. Researchers should ideally enter their protocols prospectively and update them as necessary.</p><p class=\"headingAnchor\" id=\"H110188375\"><span class=\"h2\">Publication bias</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>One of several related biases in the available evidence being considered for inclusion in a systematic review. Conceptually, studies that have been published are systematically different than studies that have failed to be published, due to lack of acceptance by journals, lack of interest by authors or research grantors, or potentially, by deliberate withholding by funders. Theoretically, &quot;positive&quot; (statistically significant) results are more likely to be published than &quot;negative&quot; results. Strictly, publication bias refers specifically to missing publications about studies.</p><p>Related biases include selective outcome reporting bias, where studies are published without certain outcomes; time-lag bias, where &quot;negative&quot; study results tend to be delayed in their publication compared with &quot;positive&quot; results; location bias, where &quot;positive&quot; or more interesting results tend to be published in journals that are more easily accessible; language bias, pertinent in certain fields, where non-English language publications differ in study results compared with those published (from the same countries or authors) in English; and multiple or duplicate publication bias, where certain studies may be overrepresented in the literature due to duplicate or overlapping publications (that may be difficult to tease apart).</p><p class=\"headingAnchor\" id=\"H110188300\"><span class=\"h2\">Q statistic (chi square test for heterogeneity)</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A statistical method to test whether studies are heterogeneous. Commonly P&lt;0.10 indicates the likely presence of statistical heterogeneity.</p><p class=\"headingAnchor\" id=\"H110188261\"><span class=\"h2\">Random effects model</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A statistical method for combining data that incorporates both within-study variances (differences in results, mostly due to chance) and the between-study variance (differences in results across studies due either to chance or to systematic differences among the studies). Assumes that each study represents a different population and that there is not a single &quot;true&quot; effect size.</p><p class=\"headingAnchor\" id=\"H2327805\"><span class=\"h2\">Sensitivity analysis</span><span class=\"headingEndMark\">&#160;&#8212;&#160;</span>A method of exploring heterogeneity<strong> </strong>in a meta-analysis by varying which studies are included to determine the effects of such changes. Used to explore how sensitive a meta-analysis finding is to inclusion of individual studies and to evaluate possible causes of heterogeneity; for example, whether exclusion of poor quality studies influences the size of the effect.</p><p class=\"headingAnchor\" id=\"H110188989\"><span class=\"h1\">SUMMARY</span></p><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>A systematic review is a comprehensive summary of all available evidence that meets predefined eligibility criteria to address a specific clinical question or range of questions. Meta-analysis, which is commonly included in systematic reviews, is a statistical method that quantitatively combines the results from different studies. It is commonly used to provide an overall estimate of the net benefit or harm of an intervention. Heterogeneity, differences among studies, may be explained by random chance alone or may be due to basic differences in trial design or participants across studies. (See <a href=\"#H2327453\" class=\"local\">'Terminology'</a> above.)</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>A systematic review should describe the research questions and the review methodology, including the search strategy and approach to analyzing the data. The literature search should be systematic and comprehensive. Several methodologies can suggest the presence of publication bias, though none are definitive. Studies should be critically appraised on the basis of multiple factors of quality to determine the degree of confidence that the estimate of effect reflects the &quot;truth&quot; as best as it can be measured. (See <a href=\"#H2326459\" class=\"local\">'Developing a protocol'</a> above.)</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>In performing a meta-analysis, the summary effect estimate can be based upon a fixed effect or random effects model; results using the random effects model are more conservative. Random effects model meta-analyses are generally preferred. (See <a href=\"#H110188885\" class=\"local\">'Meta-analysis'</a> above.)</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>The forest plot allows the reader to see the estimate and the precision of the individual studies, appreciate the heterogeneity of results, and compare the estimates of the individual studies to the overall result. Sensitivity analysis of results evaluates the stability of overall estimates among different subgroups. Heterogeneity across studies should be evaluated either qualitatively or with statistical methods including meta-regression or subgroup analysis. (See <a href=\"#H96519889\" class=\"local\">'Forest plots'</a> above and <a href=\"#H110188885\" class=\"local\">'Meta-analysis'</a> above.)</p><p/><p class=\"bulletIndent1\"><span class=\"glyph\">&#9679;</span>Systematic reviews, like primary studies, should be interpreted with questions raised about their quality and the applicability of their findings to the questions of the reader. Overall results reflect an average effect in a population and are not directly transferrable to an individual patient. (See <a href=\"#H2326530\" class=\"local\">'Reading and interpreting a systematic review'</a> above.)</p></div><div id=\"topicAgreement\">Use of UpToDate is subject to the <a href=\"https://www.uptodate.com/legal/license\" class=\"licenseLink\" id=\"sla_in_page\">Subscription and License Agreement</a>.</div><div id=\"references\" class=\"headingAnchor\"><h1>REFERENCES</h1><ol id=\"reference\"><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/1\" class=\"nounderline abstract_t\">Ioannidis JP. Why most published research findings are false: author's reply to Goodman and Greenland. PLoS Med 2007; 4:e215.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/2\" class=\"nounderline abstract_t\">LeLorier J, Gr&eacute;goire G, Benhaddad A, et al. Discrepancies between meta-analyses and subsequent large randomized, controlled trials. N Engl J Med 1997; 337:536.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/3\" class=\"nounderline abstract_t\">Cappelleri JC, Ioannidis JP, Schmid CH, et al. Large trials vs meta-analysis of smaller trials: how do their results compare? JAMA 1996; 276:1332.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/4\" class=\"nounderline abstract_t\">Villar J, Carroli G, Beliz&aacute;n JM. Predictive ability of meta-analyses of randomised controlled trials. Lancet 1995; 345:772.</a></li><li class=\"breakAll\">Systematic reviews, Chalmers I, Altman DG (Eds), BMJ Publishing Group, London 1995.</li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/6\" class=\"nounderline abstract_t\">Cook DJ, Mulrow CD, Haynes RB. Systematic reviews: synthesis of best evidence for clinical decisions. Ann Intern Med 1997; 126:376.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/7\" class=\"nounderline abstract_t\">Oxman AD, Cook DJ, Guyatt GH. Users' guides to the medical literature. VI. How to use an overview. Evidence-Based Medicine Working Group. JAMA 1994; 272:1367.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/8\" class=\"nounderline abstract_t\">Mulrow CD. The medical review article: state of the science. Ann Intern Med 1987; 106:485.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/9\" class=\"nounderline abstract_t\">Lau J, Ioannidis JP, Schmid CH. Quantitative synthesis in systematic reviews. Ann Intern Med 1997; 127:820.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/10\" class=\"nounderline abstract_t\">Liberati A, Altman DG, Tetzlaff J, et al. The PRISMA statement for reporting systematic reviews and meta-analyses of studies that evaluate health care interventions: explanation and elaboration. Ann Intern Med 2009; 151:W65.</a></li><li class=\"breakAll\">www.prisma-statement.org/Extensions/Default.aspx (Accessed on June 26, 2016).</li><li class=\"breakAll\">Institute of Medicine. Finding what workds in health care:Standards for systematic reviews. The National Academies Press, Washington, DC, 2011. Available at: http://www.iom.edu/Reports/2011/Finding-What-Works-in-Health-Care-Standards-for-Systematic-Reviews.aspx (Accessed on October 10, 2011).</li><li class=\"breakAll\">Clarke MJ, Stewart LA. Principles of and procedures for systematic reviews. In: Systematic reviews in health care: meta-analysis in context, Egger M, Smith G, Altman D (Eds), BMJ Publishing Group, London 2001. p.23.</li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/14\" class=\"nounderline abstract_t\">Counsell C. Formulating questions and locating primary studies for inclusion in systematic reviews. Ann Intern Med 1997; 127:380.</a></li><li class=\"breakAll\">http://www.ncbi.nlm.nih.gov/pubmed (Accessed on July 14, 2011).</li><li class=\"breakAll\">http://www.embase.com/search (Accessed on July 14, 2011).</li><li class=\"breakAll\">http://onlinelibrary.wiley.com/o/cochrane/cochrane_clcentral_articles_fs.html (Accessed on July 14, 2011).</li><li class=\"breakAll\">Dickersin K, Chalmers I. Recognising, investigating and dealing with incomplete and biased reporting of clinical research: from Francis Bacon to the World Health Organisation. James Lind Library 2010. Available at: www.jameslindlibrary.org (Accessed on October 10, 2011).</li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/19\" class=\"nounderline abstract_t\">Mathieu S, Boutron I, Moher D, et al. Comparison of registered and published primary outcomes in randomized controlled trials. JAMA 2009; 302:977.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/20\" class=\"nounderline abstract_t\">Kirkham JJ, Altman DG, Williamson PR. Bias due to changes in specified outcomes during the systematic review process. PLoS One 2010; 5:e9810.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/21\" class=\"nounderline abstract_t\">Ioannidis JP. Effect of the statistical significance of results on the time to completion and publication of randomized efficacy trials. JAMA 1998; 279:281.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/22\" class=\"nounderline abstract_t\">Thornton A, Lee P. Publication bias in meta-analysis: its causes and consequences. J Clin Epidemiol 2000; 53:207.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/23\" class=\"nounderline abstract_t\">Goodman S, Dickersin K. Metabias: a challenge for comparative effectiveness research. Ann Intern Med 2011; 155:61.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/24\" class=\"nounderline abstract_t\">Vevea JL, Woods CM. Publication bias in research synthesis: sensitivity analysis using a priori weight functions. Psychol Methods 2005; 10:428.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/25\" class=\"nounderline abstract_t\">Egger M, Davey Smith G, Schneider M, Minder C. Bias in meta-analysis detected by a simple, graphical test. BMJ 1997; 315:629.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/26\" class=\"nounderline abstract_t\">Tang JL, Liu JL. Misleading funnel plot for detection of bias in meta-analysis. J Clin Epidemiol 2000; 53:477.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/27\" class=\"nounderline abstract_t\">Terrin N, Schmid CH, Lau J, Olkin I. Adjusting for publication bias in the presence of heterogeneity. Stat Med 2003; 22:2113.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/28\" class=\"nounderline abstract_t\">Terrin N, Schmid CH, Lau J. In an empirical evaluation of the funnel plot, researchers could not visually identify publication bias. J Clin Epidemiol 2005; 58:894.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/29\" class=\"nounderline abstract_t\">Duval S, Tweedie R. Trim and fill: A simple funnel-plot-based method of testing and adjusting for publication bias in meta-analysis. Biometrics 2000; 56:455.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/30\" class=\"nounderline abstract_t\">Copas J. What works?: Selectivity models and meta-analysis. Journal of the Royal Statistical Society Series A 1999; 162:95.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/31\" class=\"nounderline abstract_t\">Rosenthal R. The 'file drawer problem' and tolerance for null results. Psychol Bull 1979; 86:638.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/32\" class=\"nounderline abstract_t\">Ioannidis JP, Trikalinos TA. An exploratory test for an excess of significant findings. Clin Trials 2007; 4:245.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/33\" class=\"nounderline abstract_t\">Moher D, Jadad AR, Nichol G, et al. Assessing the quality of randomized controlled trials: an annotated bibliography of scales and checklists. Control Clin Trials 1995; 16:62.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/34\" class=\"nounderline abstract_t\">Verhagen AP, de Vet HC, de Bie RA, et al. The art of quality assessment of RCTs included in systematic reviews. J Clin Epidemiol 2001; 54:651.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/35\" class=\"nounderline abstract_t\">J&uuml;ni P, Altman DG, Egger M. Systematic reviews in health care: Assessing the quality of controlled clinical trials. BMJ 2001; 323:42.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/36\" class=\"nounderline abstract_t\">J&uuml;ni P, Witschi A, Bloch R, Egger M. The hazards of scoring the quality of clinical trials for meta-analysis. JAMA 1999; 282:1054.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/37\" class=\"nounderline abstract_t\">Schulz KF, Chalmers I, Hayes RJ, Altman DG. Empirical evidence of bias. Dimensions of methodological quality associated with estimates of treatment effects in controlled trials. JAMA 1995; 273:408.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/38\" class=\"nounderline abstract_t\">Kjaergard LL, Villumsen J, Gluud C. Reported methodologic quality and discrepancies between large and small randomized trials in meta-analyses. Ann Intern Med 2001; 135:982.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/39\" class=\"nounderline abstract_t\">Moher D, Pham B, Jones A, et al. Does quality of reports of randomised trials affect estimates of intervention efficacy reported in meta-analyses? Lancet 1998; 352:609.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/40\" class=\"nounderline abstract_t\">Linde K, Scholz M, Ramirez G, et al. Impact of study quality on outcome in placebo-controlled trials of homeopathy. J Clin Epidemiol 1999; 52:631.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/41\" class=\"nounderline abstract_t\">Balk EM, Bonis PA, Moskowitz H, et al. Correlation of quality measures with estimates of treatment effect in meta-analyses of randomized controlled trials. JAMA 2002; 287:2973.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/42\" class=\"nounderline abstract_t\">Savovi&#263; J, Jones HE, Altman DG, et al. Influence of reported study design characteristics on intervention effect estimates from randomized, controlled trials. Ann Intern Med 2012; 157:429.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/43\" class=\"nounderline abstract_t\">Balk EM, Tatsioni A, Lichtenstein AH, et al. Effect of chromium supplementation on glucose metabolism and lipids: a systematic review of randomized controlled trials. Diabetes Care 2007; 30:2154.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/44\" class=\"nounderline abstract_t\">Deeks JJ. Issues in the selection of a summary statistic for meta-analysis of clinical trials with binary outcomes. Stat Med 2002; 21:1575.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/45\" class=\"nounderline abstract_t\">Walter SD. Choice of effect measure for epidemiological data. J Clin Epidemiol 2000; 53:931.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/46\" class=\"nounderline abstract_t\">Wu LA, Kottke TE. Number needed to treat: caveat emptor. J Clin Epidemiol 2001; 54:111.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/47\" class=\"nounderline abstract_t\">Engels EA, Schmid CH, Terrin N, et al. Heterogeneity and statistical significance in meta-analysis: an empirical study of 125 meta-analyses. Stat Med 2000; 19:1707.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/48\" class=\"nounderline abstract_t\">Smeeth L, Haines A, Ebrahim S. Numbers needed to treat derived from meta-analyses--sometimes informative, usually misleading. BMJ 1999; 318:1548.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/49\" class=\"nounderline abstract_t\">DerSimonian R, Laird N. Meta-analysis in clinical trials. Control Clin Trials 1986; 7:177.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/50\" class=\"nounderline abstract_t\">Lau J, Ioannidis JP, Schmid CH. Summing up evidence: one answer is not always enough. Lancet 1998; 351:123.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/51\" class=\"nounderline abstract_t\">Cornell JE, Mulrow CD, Localio R, et al. Random-effects meta-analysis of inconsistent effects: a time for change. Ann Intern Med 2014; 160:267.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/52\" class=\"nounderline abstract_t\">Bradburn MJ, Deeks JJ, Berlin JA, Russell Localio A. Much ado about nothing: a comparison of the performance of meta-analytical methods with rare events. Stat Med 2007; 26:53.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/53\" class=\"nounderline abstract_t\">Higgins JP, Thompson SG, Deeks JJ, Altman DG. Measuring inconsistency in meta-analyses. BMJ 2003; 327:557.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/54\" class=\"nounderline abstract_t\">Huedo-Medina TB, S&aacute;nchez-Meca J, Mar&iacute;n-Mart&iacute;nez F, Botella J. Assessing heterogeneity in meta-analysis: Q statistic or I2 index? Psychol Methods 2006; 11:193.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/55\" class=\"nounderline abstract_t\">Guyatt GH, Oxman AD, Kunz R, et al. GRADE guidelines 6. Rating the quality of evidence--imprecision. J Clin Epidemiol 2011; 64:1283.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/56\" class=\"nounderline abstract_t\">Dechartres A, Altman DG, Trinquart L, et al. Association between analytic strategy and estimates of treatment outcomes in meta-analyses. JAMA 2014; 312:623.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/57\" class=\"nounderline abstract_t\">Berlin JA, Golub RM. Meta-analysis as evidence: building a better pyramid. JAMA 2014; 312:603.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/58\" class=\"nounderline abstract_t\">Berkey CS, Hoaglin DC, Mosteller F, Colditz GA. A random-effects regression model for meta-analysis. Stat Med 1995; 14:395.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/59\" class=\"nounderline abstract_t\">Schmid CH. Exploring heterogeneity in randomized trials via metaanalysis. Drug Inf J 1999; 33:211.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/60\" class=\"nounderline abstract_t\">van Houwelingen HC, Arends LR, Stijnen T. Advanced methods in meta-analysis: multivariate approach and meta-regression. Stat Med 2002; 21:589.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/61\" class=\"nounderline abstract_t\">Ioannidis JP, Cappelleri JC, Sacks HS, Lau J. The relationship between study design, results, and reporting of randomized clinical trials of HIV infection. Control Clin Trials 1997; 18:431.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/62\" class=\"nounderline abstract_t\">Schulz KF, Grimes DA. Multiplicity in randomised trials II: subgroup and interim analyses. Lancet 2005; 365:1657.</a></li><li class=\"breakAll\">Rothman KJ, Greenland S. Modern epidemiology, 2nd ed, Lippincott-Raven, Philadelphia 1998.</li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/64\" class=\"nounderline abstract_t\">Povitz M, Bolo CE, Heitman SJ, et al. Effect of treatment of obstructive sleep apnea on depressive symptoms: systematic review and meta-analysis. PLoS Med 2014; 11:e1001762.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/65\" class=\"nounderline abstract_t\">Jansen JP, Fleurence R, Devine B, et al. Interpreting indirect treatment comparisons and network meta-analysis for health-care decision making: report of the ISPOR Task Force on Indirect Treatment Comparisons Good Research Practices: part 1. Value Health 2011; 14:417.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/66\" class=\"nounderline abstract_t\">Mills EJ, Ioannidis JP, Thorlund K, et al. How to use an article reporting a multiple treatment comparison meta-analysis. JAMA 2012; 308:1246.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/67\" class=\"nounderline abstract_t\">Bangalore S, Kumar S, Kjeldsen SE, et al. Antihypertensive drugs and risk of cancer: network meta-analyses and trial sequential analyses of 324,168 participants from randomised trials. Lancet Oncol 2011; 12:65.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/68\" class=\"nounderline abstract_t\">Salanti G. Indirect and mixed-treatment comparison, network, or multiple-treatments meta-analysis: many names, many benefits, many concerns for the next generation evidence synthesis tool. Res Synth Methods 2012; 3:80.</a></li><li><a href=\"https://www.uptodate.com/contents/systematic-review-and-meta-analysis/abstract/69\" class=\"nounderline abstract_t\">Trinquart L, Attiche N, Bafeta A, et al. Uncertainty in Treatment Rankings: Reanalysis of Network Meta-analyses of Randomized Trials. Ann Intern Med 2016; 164:666.</a></li></ol></div><div id=\"topicVersionRevision\">Topic 16293 Version 19.0</div></div>","outline":"<div id=\"outlineSections\"><h2>Topic Outline</h2><ul id=\"innerOutline\"><li class=\"sr-button\"><a href=\"#H110188989\"><span>SUMMARY</span></a></li><li><a href=\"#H2326445\" id=\"outline-link-H2326445\">INTRODUCTION</a></li><li><a href=\"#H2327453\" id=\"outline-link-H2327453\">TERMINOLOGY</a><ul><li><a href=\"#H78553622\" id=\"outline-link-H78553622\">Systematic review</a></li><li><a href=\"#H78553629\" id=\"outline-link-H78553629\">Meta-analysis</a></li><li><a href=\"#H78553636\" id=\"outline-link-H78553636\">Heterogeneity</a></li></ul></li><li><a href=\"#H2326452\" id=\"outline-link-H2326452\">OVERVIEW OF SYSTEMATIC REVIEW AND META-ANALYSIS</a><ul><li><a href=\"#H2327460\" id=\"outline-link-H2327460\">Consensus statements</a></li></ul></li><li><a href=\"#H2326459\" id=\"outline-link-H2326459\">DEVELOPING A PROTOCOL</a><ul><li><a href=\"#H2326466\" id=\"outline-link-H2326466\">Formulating research questions</a></li></ul></li><li><a href=\"#H2326473\" id=\"outline-link-H2326473\">PERFORMING THE LITERATURE SEARCH</a><ul><li><a href=\"#H2326480\" id=\"outline-link-H2326480\">Publication bias</a></li></ul></li><li><a href=\"#H2326487\" id=\"outline-link-H2326487\">ASSESSING THE QUALITY OF STUDIES</a></li><li><a href=\"#H96519889\" id=\"outline-link-H96519889\">FOREST PLOTS</a></li><li><a href=\"#H110188885\" id=\"outline-link-H110188885\">META-ANALYSIS</a><ul><li><a href=\"#H2326494\" id=\"outline-link-H2326494\">Statistical methods for combining data</a></li><li><a href=\"#H2326501\" id=\"outline-link-H2326501\">When to combine studies</a></li><li><a href=\"#H7125075\" id=\"outline-link-H7125075\">Precision</a></li><li><a href=\"#H2327699\" id=\"outline-link-H2327699\">Sensitivity analysis</a></li><li><a href=\"#H2327476\" id=\"outline-link-H2327476\">Meta-regression</a></li><li><a href=\"#H2327483\" id=\"outline-link-H2327483\">Subgroup analyses</a></li><li><a href=\"#H2326516\" id=\"outline-link-H2326516\">Individual patient data</a></li><li><a href=\"#H2326523\" id=\"outline-link-H2326523\">Network meta-analysis</a></li></ul></li><li><a href=\"#H2326530\" id=\"outline-link-H2326530\">READING AND INTERPRETING A SYSTEMATIC REVIEW</a></li><li><a href=\"#H2326537\" id=\"outline-link-H2326537\">GLOSSARY OF TERMS</a><ul><li><a href=\"#H110188417\" id=\"outline-link-H110188417\">Applicability (generalizability)</a></li><li><a href=\"#H2327798\" id=\"outline-link-H2327798\">Ecological fallacy (ecological inference fallacy)</a></li><li><a href=\"#H110188273\" id=\"outline-link-H110188273\">Fixed effect model</a></li><li><a href=\"#H110188336\" id=\"outline-link-H110188336\">Forest plot</a></li><li><a href=\"#H110188387\" id=\"outline-link-H110188387\">Funnel plot</a></li><li><a href=\"#H110188399\" id=\"outline-link-H110188399\">Grey literature</a></li><li><a href=\"#H110188136\" id=\"outline-link-H110188136\">Heterogeneity (statistical heterogeneity)</a><ul><li><a href=\"#H110188185\" id=\"outline-link-H110188185\">- Clinical heterogeneity</a></li></ul></li><li><a href=\"#H110188287\" id=\"outline-link-H110188287\">I2 index</a></li><li><a href=\"#H110188249\" id=\"outline-link-H110188249\">Key questions</a></li><li><a href=\"#H110188324\" id=\"outline-link-H110188324\">Meta-regression</a></li><li><a href=\"#H110188312\" id=\"outline-link-H110188312\">Network meta-analysis</a></li><li><a href=\"#H110188348\" id=\"outline-link-H110188348\">PICO method (PICOD, PICOS, PICOTS, others)</a></li><li><a href=\"#H110189133\" id=\"outline-link-H110189133\">Precision</a></li><li><a href=\"#H110188452\" id=\"outline-link-H110188452\">PRISMA statement</a></li><li><a href=\"#H587111879\" id=\"outline-link-H587111879\">PROSPERO</a></li><li><a href=\"#H110188375\" id=\"outline-link-H110188375\">Publication bias</a></li><li><a href=\"#H110188300\" id=\"outline-link-H110188300\">Q statistic (chi square test for heterogeneity)</a></li><li><a href=\"#H110188261\" id=\"outline-link-H110188261\">Random effects model</a></li><li><a href=\"#H2327805\" id=\"outline-link-H2327805\">Sensitivity analysis</a></li></ul></li><li><a href=\"#H110188989\" id=\"outline-link-H110188989\">SUMMARY</a></li><li><a href=\"#references\">REFERENCES</a></li></ul></div><div><h2>GRAPHICS <a href=\"#\" id=\"viewAllGraphicsLink\">View All</a></h2><div id=\"outlineGraphics\"><ul><li><div id=\"PEDS/16293|FIG\" class=\"openRelatedGraphics\"><a href=\"#\" title=\"FIGURES\">FIGURES</a></div><ul><li><a href=\"image.htm?imageKey=PC/65012\" class=\"graphic graphic_figure\">- Funnel plots</a></li><li><a href=\"image.htm?imageKey=PC/59845\" class=\"graphic graphic_figure\">- Forest plot</a></li><li><a href=\"image.htm?imageKey=PC/80325\" class=\"graphic graphic_figure\">- Fixed effects model</a></li><li><a href=\"image.htm?imageKey=PC/51482\" class=\"graphic graphic_figure\">- Random effects model</a></li><li><a href=\"image.htm?imageKey=PC/52219\" class=\"graphic graphic_figure\">- Precision and validity</a></li><li><a href=\"image.htm?imageKey=PC/69060\" class=\"graphic graphic_figure\">- Meta-regression</a></li><li><a href=\"image.htm?imageKey=PC/63627\" class=\"graphic graphic_figure\">- Ecological fallacy</a></li><li><a href=\"image.htm?imageKey=PC/104212\" class=\"graphic graphic_figure\">- CPAP studies forest plot</a></li><li><a href=\"image.htm?imageKey=PC/79458\" class=\"graphic graphic_figure\">- Network meta-analysis of antihypertensive drugs</a></li></ul></li><li><div id=\"PEDS/16293|TAB\" class=\"openRelatedGraphics\"><a href=\"#\" title=\"TABLES\">TABLES</a></div><ul><li><a href=\"image.htm?imageKey=PC/80657\" class=\"graphic graphic_table\">- Methods of summarizing studies in systematic reviews</a></li><li><a href=\"image.htm?imageKey=PC/104213\" class=\"graphic graphic_table\">- Meta-regression analysis for effect of CPAP</a></li><li><a href=\"image.htm?imageKey=PC/56270\" class=\"graphic graphic_table\">- Questions to consider</a></li></ul></li></ul></div></div><div><h2>RELATED TOPICS</h2><div id=\"outlineTopics\"><ul><li class=\"plainItem\"><a href=\"topic.htm?path=glossary-of-common-biostatistical-and-epidemiological-terms\" class=\"medical medical_review\">Glossary of common biostatistical and epidemiological terms</a></li></ul></div></div>","javascript":null}